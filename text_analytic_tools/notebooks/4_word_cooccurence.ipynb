{"cells":[{"cell_type":"markdown","metadata":{},"source":["## tCoIR - Text Analysis\n","### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"]},{"cell_type":"code","execution_count":5,"metadata":{"code_folding":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":"# Setup\n%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport pandas as pd\nimport text_analytic_tools.utility as utility\nimport text_analytic_tools.utility.widgets as widgets\nimport text_analytic_tools.common.text_corpus as text_corpus\nimport text_analytic_tools.common.textacy_utility as textacy_utility\n\nfrom beakerx.object import beakerx\nfrom beakerx import *\nfrom IPython.display import display #, set_matplotlib_formats\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom text_analytic_tools.domain_logic_config import current_domain as domain_logic\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nlogger = utility.getLogger('corpus_text_analysis')\n\nutility.setup_default_pd_display(pd)\n"},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green'>PREPARE </span> HAL Co-Windows Ratio (CWR)<span style='float: right; color: red'>MANDATORY</span>\n","\n","Term \"HAL\" co-occurrence frequencies is calculated in accordance with Hyperspace Analogue to Language (Lund; Burgess, 1996) vector-space model. The computation is specified in detail in section 3.1 in (Chen; Lu, 2011).\n","\n","\\begin{aligned}\n","nw(x) &= \\text{number of sliding windows that contains term $x$} \\\\\n","nw(x, y) &= \\text{number of sliding windows that contains $x$ and $y$} \\\\\n","\\\\\n","f(x, y) &= \\text{normalized version of nw(x, y)} \\\\\n","CWR(x, y) &= \\frac{nw(x, y)}{nw(x) + nw(y) - nw(x, y)}\\\\\n","\\end{aligned}\n","\n","- Chen Z.; Lu Y., \"A Word Co-occurrence Matrix Based Method for Relevance Feedback\"\n","- Lund, K.; Burgess, C. & Atchley, R. A. (1995). \"Semantic and associative priming in high-dimensional semantic space\".[Link](https://books.google.de/books?id=CSU_Mj07G7UC).\n","- Lund, K.; Burgess, C. (1996). \"Producing high-dimensional semantic spaces from lexical co-occurrence\". doi:10.3758/bf03204766 [Link](https://dx.doi.org/10.3758%2Fbf03204766).\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"import itertools\nimport glove\nimport pandas as pd\nimport collections\n\ndef build_vocab(corpus):\n    ''' Iterates corpus and add distict terms to vocabulary '''\n    logger.info('Builiding vocabulary...')\n    token2id = collections.defaultdict()\n    token2id.default_factory = token2id.__len__\n    for doc in corpus:\n        for term in doc:\n            token2id[term]\n    logger.info('Vocabulary of size {} built.'.format(len(token2id)))\n    return token2id\n\n# See http://www.foldl.me/2014/glove-python/\nclass GloveVectorizer():\n    \n    def __init__(self, corpus=None, token2id=None):\n        \n        self.token2id = token2id\n        self._id2token = None\n        self.corpus = corpus        \n        \n    @property\n    def corpus(self):\n        return self._corpus\n    \n    @corpus.setter\n    def corpus(self, value):\n    \n        self._corpus = value\n        self.term_count = sum(map(len, value or []))\n        \n        if self.token2id is None and value is not None:\n            self.token2id = build_vocab(value)\n            self._id2token = None\n    \n    @property\n    def id2token(self):\n        if self._id2token is None:\n            if self.token2id is not None:\n                self._id2token = { v:k for k,v in self.token2id.items() }\n        return self._id2token\n    \n    #def fit(self, sentences, window=2, dictionary=None):\n    def fit(self, corpus=None, size=2):  #, distance_metric=0, zero_out_diag=False):\n        \n        if corpus is not None:\n            self.corpus = corpus\n            \n        assert self.token2id is not None, \"Fit with no vocabulary!\"\n        assert self.corpus is not None, \"Fit with no corpus!\"\n        \n        glove_corpus = glove.Corpus(dictionary=self.token2id)\n        glove_corpus.fit(corpus, window=size)\n\n        self.nw_xy = glove_corpus.matrix\n        \n        return self\n    \n    def cooccurence(self, normalize='size', zero_diagonal=True):\n        '''Return computed co-occurrence values'''\n        \n        matrix = self.nw_xy\n        \n        #if zero_diagonal:\n        #    matrix.fill_diagonal(0)\n                \n        coo_matrix = matrix #.tocoo(copy=False)\n        \n        df = pd.DataFrame({\n            'x_id': coo_matrix.row,\n            'y_id': coo_matrix.col,\n            'nw_xy': coo_matrix.data,\n            'nw_x': 0,\n            'nw_y': 0,\n        }).reset_index(drop=True)\n        \n        df = df.assign(\n            x_term=df.x_id.apply(lambda x: self.id2token[x]),\n            y_term=df.y_id.apply(lambda x: self.id2token[x])\n        )\n        \n        df = df[['x_id', 'y_id', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y']]\n        \n        norm = 1.0\n        if normalize == 'size':\n            norm = self.term_count\n        elif normalize == 'max':\n            norm = np.max(coo_matrix)\n        elif normalize is None:\n            logger.warning('No normalize method specified. Using absolute counts...')\n            pass # return as as is...\"\n        else:\n            assert False, 'Unknown normalize specifier'\n\n        df_nw_xy = df.assign(cwr=df.nw_xy / norm)\n        \n        return df_nw_xy[df_nw_xy.cwr > 0]\n"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2019-09-01 10:18:23,050 : INFO : Builiding vocabulary...\n","2019-09-01 10:18:23,052 : INFO : Vocabulary of size 7 built.\n","2019-09-01 10:18:23,059 : INFO : Builiding vocabulary...\n","2019-09-01 10:18:23,060 : INFO : Vocabulary of size 6 built.\n"]},{"name":"stdout","output_type":"stream","text":["Test run OK\n"]}],"source":"import sys, array, collections\nimport scipy.sparse as sp\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport text_analytic_tools.utility as utility\n\nfrom glove import Corpus \n\nlogger = utility.getLogger('corpus_text_analysis')\n\nclass HyperspaceAnalogueToLanguageVectorizer():\n    \n    def __init__(self, corpus=None, token2id=None, tick=utility.noop):\n        \"\"\"\n        Build vocabulary and create nw_xy term-term matrix and nw_x term global occurence vector\n        \n        Parameter:\n            corpus Iterable[Iterable[str]]\n\n        \"\"\"\n        self.token2id = token2id\n        self._id2token = None\n        self.corpus = corpus\n        \n        self.nw_xy = None\n        self.nw_x = None\n        self.tick = tick\n\n    @property\n    def corpus(self):\n        return self._corpus\n    \n    @corpus.setter\n    def corpus(self, value):\n    \n        self._corpus = value\n        self.term_count = sum(map(len, value or []))\n        \n        if self.token2id is None and value is not None:\n            self.token2id = build_vocab(value)\n            self._id2token = None\n    \n    @property\n    def id2token(self):\n        if self._id2token is None:\n            if self.token2id is not None:\n                self._id2token = { v:k for k,v in self.token2id.items() }\n        return self._id2token\n    \n    def sliding_window(self, seq, n):\n        it = itertools.chain(iter(seq), [None] * n)\n        memory = tuple(itertools.islice(it, n+1))\n        if len(memory) == n+1:\n            yield memory\n        for x in it:\n            memory = memory[1:] + (x,)\n            yield memory\n\n    def fit(self, corpus=None, size=2, distance_metric=0, zero_out_diag=False):\n        \n        '''Trains HAL for a document. Note that sentence borders (for now) are ignored'''\n        \n        if corpus is not None:\n            self.corpus = corpus\n            \n        assert self.token2id is not None, \"Fit with no vocabulary!\"\n        assert self.corpus is not None, \"Fit with no corpus!\"\n\n        nw_xy = sp.lil_matrix ((len(self.token2id), len(self.token2id)), dtype=np.int32)\n        nw_x = np.zeros(len(self.token2id), dtype=np.int32)\n        \n        for terms in corpus:\n            \n            id_terms = ( self.token2id[size] for size in terms)\n            \n            self.tick()\n            \n            for win in self.sliding_window(id_terms, size):\n                \n                #logger.info([ self.id2token[x] if x is not None else None for x in win])\n                \n                if win[0] is None:\n                    continue\n                    \n                for x in win:\n                    if x is not None:\n                        nw_x[x] += 1\n\n                for i in range(1, size+1):\n\n                    if win[i] is None:\n                        continue\n                        \n                    if zero_out_diag:\n                        if win[0] == win[i]:\n                            continue\n                        \n                    d = float(i) # abs(n - i)\n                    if distance_metric == 0: #  linear i.e. adjacent equals window size, then decreasing by one\n                        w = (size - d + 1) # / size\n                    elif distance_metric == 1: # f(d) = 1 / d\n                        w = 1.0 / d\n                    elif distance_metric == 2: # Constant value of 1\n                        w = 1\n\n                    #print('*', i, self.id2token[win[0]], self.id2token[win[i]], w, [ self.id2token[x] if x is not None else None for x in win])\n                    nw_xy[win[0], win[i]] += w\n                    \n        self.nw_x = nw_x\n        self.nw_xy = nw_xy\n        #self.f_xy = nw_xy / np.max(nw_xy)\n\n        return self\n    \n    def to_df(self):\n        columns = [ self.id2token[i] for i in range(0,len(self.token2id))]\n        return pd.DataFrame(\n            data=self.nw_xy.todense(),\n            index=list(columns),\n            columns=list(columns),\n            dtype=np.float64\n        ).T\n    \n    def cwr(self, direction_sensitive=False, normalize='size'):\n\n        n = self.nw_x.shape[0]\n        \n        nw = self.nw_x.reshape(n,1)\n        nw_xy = self.nw_xy\n        \n        norm = 1.0\n        if normalize == 'size':\n            norm = float(self.term_count)\n        elif norm == 'max':\n            norm = float(np.max(nw_xy))\n        elif norm == 'sum':\n            norm = float(np.sum(nw_xy))\n        \n        #nw.resize(nw.shape[0], 1)\n        \n        self.cwr = sp.lil_matrix(nw_xy / (-nw_xy + nw + nw.T)) #nw.reshape(n,1).T))\n        \n        if norm != 1.0:\n            self.cwr = self.cwr / norm\n            \n        coo_matrix = self.cwr.tocoo(copy=False)\n        df = pd.DataFrame({\n            'x_id': coo_matrix.row,\n            'y_id': coo_matrix.col,\n            'cwr': coo_matrix.data\n        }).sort_values(['x_id', 'y_id']).reset_index(drop=True)\n        \n        df = df.assign(\n            x_term=df.x_id.apply(lambda x: self.id2token[x]),\n            y_term=df.y_id.apply(lambda x: self.id2token[x])\n        )\n        \n        df_nw_x = pd.DataFrame(self.nw_x, columns=['nw'])\n        \n        df = df.merge(df_nw_x, left_on='x_id', right_index=True, how='inner').rename(columns={'nw': 'nw_x'})\n        df = df.merge(df_nw_x, left_on='y_id', right_index=True, how='inner').rename(columns={'nw': 'nw_y'})\n        \n        df = df[['x_id', 'y_id', 'x_term', 'y_term', 'cwr']]\n        \n        return df\n    \n    def cooccurence2(self, direction_sensitive=False, normalize='size', zero_diagonal=True):\n        n = self.cwr.shape[0]\n        df = pd.DataFrame([(\n                i,\n                j,\n                self.id2token[i],\n                self.id2token[j],\n                self.nw_xy[i,j],\n                self.nw_x[i],\n                self.nw_x[j],\n                self.cwr[i,j]\n            ) for i,j in itertools.product(range(0,n), repeat=2) if self.cwr[i,j] > 0 ], columns=['x_id', 'y_id', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y', 'cwr'])\n        \n        return df    \n    \n    def cooccurence(self, direction_sensitive=False, normalize='size', zero_diagonal=True):\n        '''Return computed co-occurrence values'''\n        \n        matrix = self.nw_xy\n        \n        if not direction_sensitive:\n            matrix += matrix.T\n            matrix[np.tril_indices(matrix.shape[0])] = 0\n        else:\n            if zero_diagonal:\n                matrix.fill_diagonal(0)\n                \n        coo_matrix = matrix.tocoo(copy=False)\n        \n        df_nw_x = pd.DataFrame(self.nw_x, columns=['nw'])\n        \n        df = pd.DataFrame({\n            'x_id': coo_matrix.row,\n            'y_id': coo_matrix.col,\n            'nw_xy': coo_matrix.data\n        })[['x_id', 'y_id', 'nw_xy']].sort_values(['x_id', 'y_id']).reset_index(drop=True)\n        \n        df = df.assign(\n            x_term=df.x_id.apply(lambda x: self.id2token[x]),\n            y_term=df.y_id.apply(lambda x: self.id2token[x])\n        )\n        \n        df = df.merge(df_nw_x, left_on='x_id', right_index=True, how='inner').rename(columns={'nw': 'nw_x'})\n        df = df.merge(df_nw_x, left_on='y_id', right_index=True, how='inner').rename(columns={'nw': 'nw_y'})\n        \n        df = df[['x_id', 'y_id', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y']]\n        \n        norm = 1.0\n        if normalize == 'size':\n            norm = self.term_count\n        elif normalize == 'max':\n            norm = np.max(coo_matrix)\n        elif normalize is None:\n            logger.warning('No normalize method specified. Using absolute counts...')\n            pass # return as as is...\"\n        else:\n            assert False, 'Unknown normalize specifier'\n\n        #logger.info('Normalizing for document corpus size %s.', norm)\n\n        df_nw_xy = df.assign(cwr=((df.nw_xy / (df.nw_x + df.nw_y - df.nw_xy)) / norm))\n\n        df_nw_xy.loc[df_nw_xy.cwr < 0.0, 'cwr'] = 0\n        df_nw_xy.cwr.fillna(0.0, inplace=True)\n        \n        return df_nw_xy[df_nw_xy.cwr > 0]\n\ndef test_burgess_litmus_test():\n    terms = 'The Horse Raced Past The Barn Fell .'.lower().split()\n    answer = {\n     'barn':  {'.': 4,  'barn': 0,  'fell': 5,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n     'fell':  {'.': 5,  'barn': 0,  'fell': 0,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n     'horse': {'.': 0,  'barn': 2,  'fell': 1,  'horse': 0,  'past': 4,  'raced': 5,  'the': 3},\n     'past':  {'.': 2,  'barn': 4,  'fell': 3,  'horse': 0,  'past': 0,  'raced': 0,  'the': 5},\n     'raced': {'.': 1,  'barn': 3,  'fell': 2,  'horse': 0,  'past': 5,  'raced': 0,  'the': 4},\n     'the':   {'.': 3,  'barn': 6,  'fell': 4,  'horse': 5,  'past': 3,  'raced': 4,  'the': 2}\n    }\n    df_answer = pd.DataFrame(answer).astype(np.int32)[['the', 'horse', 'raced', 'past', 'barn', 'fell']].sort_index()\n    #display(df_answer)\n    vectorizer = HyperspaceAnalogueToLanguageVectorizer()\n    vectorizer.fit([terms], size=5, distance_metric=0)\n    df_imp = vectorizer.to_df().astype(np.int32)[['the', 'horse', 'raced', 'past', 'barn', 'fell']].sort_index()\n    assert df_imp.equals(df_answer), \"Test failed\"\n    #df_imp == df_answer\n\n    # Example in Chen, Lu:\n    terms = 'The basic concept of the word association'.lower().split()\n    vectorizer = HyperspaceAnalogueToLanguageVectorizer().fit([terms], size=5, distance_metric=0)\n    df_imp = vectorizer.to_df().astype(np.int32)[['the', 'basic', 'concept', 'of', 'word', 'association']].sort_index()\n    df_answer = pd.DataFrame({\n        'the': [2, 5, 4, 3, 6, 4],\n        'basic': [3, 0, 5, 4, 2, 1],\n        'concept': [4, 0, 0, 5, 3, 2], \n        'of': [5, 0, 0, 0, 4, 3],\n        'word': [0, 0, 0, 0, 0, 5],\n        'association': [0, 0, 0, 0, 0, 0]\n        },\n        index=['the', 'basic', 'concept', 'of', 'word', 'association'],\n        dtype=np.int32\n    ).sort_index()[['the', 'basic', 'concept', 'of', 'word', 'association']]\n    assert df_imp.equals(df_answer), \"Test failed\"\n    print('Test run OK')\n    \n    \n    \ntest_burgess_litmus_test()\n"},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green'>PREPARE </span> Compute Using Prepared Tokenized Corpus <span style='float: right; color: red'>MANDATORY</span>\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4d7b73c6f8a46568fc6378658682721","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Corpus', layout=Layout(width='400px'), opti…"]},"metadata":{},"output_type":"display_data"}],"source":"import ipywidgets\nimport text_corpus\nimport time\n\nclass PreparedCorpusUserInterface():\n    \n    def __init__(self, data_folder):\n        self.data_folder = data_folder\n        \n    def display(self, compute_handler):\n        \n        def on_button_clicked(b):\n            \n            if self.filepath.value is None:\n                return\n            \n            self.out.clear_output()\n            with self.out:\n                self.button.disabled = True\n                compute_handler(\n                    self.filepath.value,\n                    window_size=self.window_size.value,\n                    distance_metric=self.distance_metric.value,\n                    direction_sensitive=False, # self.direction_sensitive.value,\n                    method=self.method.value\n                )\n                self.button.disabled = False\n\n        corpus_files = sorted(glob.glob(os.path.join(self.data_folder, '*.tokenized.zip')))\n        distance_metric_options = [\n            ('linear', 0),\n            ('inverse', 1),\n            ('constant', 2)\n        ]\n        \n        self.filepath            = widgets.Dropdown(description='Corpus', options=corpus_files, value=None, layout=widgets.Layout(width='400px'))\n        self.window_size         = widgets.IntSlider(description='Window', min=2, max=40, value=5, layout=widgets.Layout(width='250px'))\n        self.method              = widgets.Dropdown(description='Method', options=['HAL', 'Glove'], value='HAL', layout=widgets.Layout(width='200px'))\n        self.button              = widgets.Button(description='Compute', button_style='Success', layout=widgets.Layout(width='115px',background_color='blue'))\n        self.out                 = widgets.Output()\n        \n        self.distance_metric     = widgets.Dropdown(description='Dist.f.', options=distance_metric_options, value=2, layout=widgets.Layout(width='200px'))\n        #self.direction_sensitive = widgets.ToggleButton(description='L/R', value=False, layout=widgets.Layout(width='115px',background_color='blue'))\n        #self.zero_diagonal       = widgets.ToggleButton(description='Zero Diag', value=False, layout=widgets.Layout(width='115px',background_color='blue'))\n        \n        self.button.on_click(on_button_clicked)\n        \n        return widgets.VBox([\n            widgets.HBox([\n                widgets.VBox([\n                    self.filepath,\n                    self.method\n                ]),\n                widgets.VBox([\n                    self.window_size,\n                    self.distance_metric\n                ]),\n                widgets.VBox([\n                    #self.direction_sensitive,\n                    self.button\n                ])\n            ]),\n            self.out])\n    \nimport re\ndef source_corpus_filename(tokenized_corpus_name):\n    try:\n        m = re.match('(.*\\.txt)_preprocessed.*', tokenized_corpus_name)\n        return m.groups()[0] + '.zip'\n    except:\n        return None\n\n#FIXME: UNESCO-specific logic\ndef get_source_specific_index(source_name):\n    source_index = domain_logic.load_corpus_index(source_name)\n    if source_index is not None:\n        source_index = source_index.set_index('local_number')\n    return source_index\n    \ndef do_some_stuff(\n    filepath,\n    window_size=5,\n    distance_metric=0,\n    direction_sensitive=False,\n    normalize='size',\n    zero_diagonal=True,\n    method='HAL'\n):\n\n    corpus = text_corpus.SimplePreparedTextCorpus(filepath, lowercase=True)\n    doc_terms = [ [ t.lower().strip('_') for t in terms if len(t) > 2] for terms in corpus.get_texts() ]\n    \n    common_token2id = build_vocab(doc_terms)\n    \n    source_name = source_corpus_filename(filepath)\n    document_index = domain_logic.compile_documents(corpus)\n    \n    source_index = get_source_specific_index(source_name)\n    #threshold = 0.005\n    \n    dfs = []\n    min_year, max_year = document_index.year.min(),  document_index.year.max()\n    document_index['sequence_id'] = range(0, len(document_index))\n\n    for year in range(min_year, max_year + 1):\n        \n        year_indexes = list(document_index.loc[document_index.year == year].sequence_id)\n        \n        docs = [ doc_terms[y] for y in year_indexes ]\n        \n        logger.info('Year %s...', year)\n        \n        if method == \"HAL\":\n            \n            vectorizer = HyperspaceAnalogueToLanguageVectorizer(token2id=common_token2id)\\\n                .fit(docs, size=window_size, distance_metric=distance_metric)\n        \n            df = vectorizer.cooccurence(direction_sensitive=direction_sensitive, normalize=normalize, zero_diagonal=zero_diagonal)\n            \n        else:\n            \n            vectorizer = GloveVectorizer(token2id=common_token2id)\\\n                .fit(docs, size=window_size)\n            \n            df = vectorizer.cooccurence(normalize=normalize, zero_diagonal=zero_diagonal)\n            \n        df['year'] = year\n        #df = df[df.cwr >= threshhold]\n        \n        dfs.append(df[['year', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y', 'cwr']])\n\n        #if i == 5: break\n            \n    df = pd.concat(dfs, ignore_index=True)\n\n    df['cwr'] = df.cwr / np.max(df.cwr, axis=0)\n    #display(df.sort_values('cwr', ascending=False).head(100))\n\n    result_filename = '{}_{}_result_co_occurrence_{}.xlsx'.format(method, window_size, time.strftime(\"%Y%m%d_%H%M%S\"))\n    df.to_excel(result_filename)\n    print('Result saved to file {}'.format(result_filename))\n    \n    print('Now you are ready to do some serious stuff!')\n    #return doc_terms\n    \ndisplay(PreparedCorpusUserInterface(domain_logic.DATA_FOLDER).display(do_some_stuff))\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}
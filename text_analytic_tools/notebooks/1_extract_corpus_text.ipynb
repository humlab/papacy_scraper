{"cells":[{"cell_type":"markdown","metadata":{},"source":["## tCoIR - Text Analysis\n","### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"code_folding":[]},"outputs":[],"source":"# Setup\n%load_ext autoreload\n%autoreload 2\n\nimport sys, os, collections, zipfile\nimport re, typing.re\nimport nltk, textacy, spacy \nimport ipywidgets\nimport text_analytic_tools.utility as utility\nimport text_analytic_tools.utility.widgets as widgets\nimport text_analytic_tools.common.text_corpus as text_corpus\nimport text_analytic_tools.common.textacy_utility as textacy_utility\n\nfrom beakerx.object import beakerx\nfrom beakerx import *\nfrom IPython.display import display\n\nlogger = utility.getLogger('corpus_text_analysis')\n\nutility.setup_default_pd_display(pd)\n\ncurrent_corpus_container = lambda: textacy_utility.CorpusContainer.container()\ncurrent_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n\nfrom text_analytic_tools.domain_logic_config import current_domain as domain_logic\n"},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import textacy_corpus_gui\n\ntry:\n    container = current_corpus_container()\n    textacy_corpus_gui.display_corpus_load_gui(domain_logic.DATA_FOLDER, container=container, domain_logic=domain_logic)\nexcept Exception as ex:\n    raise\n    logger.error(ex)\n"},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green'>PREPARE </span> Extract Text From Corpus <span style='float: right; color: green'>TRY IT</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"code_folding":[]},"outputs":[],"source":"import gui_utility\nimport textacy_corpus_utility as textacy_utility\n#import domain_logic_vatican as domain_logic\nfrom domain_logic_config import current_domain as domain_logic\n\nDF_TAGSET = pd.read_csv(os.path.join(domain_logic.DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n\ndef chunks(l, n):\n    '''Returns list l in n-sized chunks'''\n    if (n or 0) == 0:\n        yield l\n    else:\n        for i in range(0, len(l), n):\n            yield l[i:i + n]\n\ndef tokenize_docs(docs, **opts): \n    try:\n        document_id = 0\n        normalize = opts['normalize'] or 'orth'\n        term_substitutions = opts.get('substitutions', {})\n        word_counts = opts.get('word_counts', {})\n        word_document_counts = opts.get('word_document_counts', {})\n        extra_stop_words = set([])\n\n        if opts['min_freq'] > 1:\n            stop_words = utility.extract_counter_items_within_threshold(word_counts[normalize], 1, opts['min_freq'])\n            extra_stop_words.update(stop_words)\n\n        if opts['max_doc_freq'] < 100:\n            stop_words = utility.extract_counter_items_within_threshold(word_document_counts[normalize], opts['max_doc_freq'], 100)\n            extra_stop_words.update(stop_words)\n\n        extract_args = dict(\n            args=dict(\n                ngrams=opts['ngrams'],\n                named_entities=opts['named_entities'],\n                normalize=opts['normalize'],\n                as_strings=True\n            ),\n            kwargs=dict(\n                min_freq=opts['min_freq'],\n                include_pos=opts['include_pos'],\n                filter_stops=opts['filter_stops'],\n                filter_punct=opts['filter_punct']\n            ),\n            extra_stop_words=extra_stop_words,\n            substitutions=(term_substitutions if opts.get('substitute_terms', False) else None),\n        )\n\n        for document_name, doc in docs:\n            print(document_name)\n\n            terms = [ x for x in textacy_utility.extract_document_terms(doc, extract_args)]\n            \n            chunk_size = opts.get('chunk_size', 0)\n            chunk_index = 0\n            for tokens in chunks(terms, chunk_size):\n                yield document_id, document_name, chunk_index, tokens\n                chunk_index += 1\n\n            document_id += 1\n                    \n    except Exception as ex:\n        raise\n        logger.error(ex)\n        \ndef store_tokenized_corpus(tokenized_docs, corpus_source_filepath, **opts): \n    \n    filepath = utility.path_add_timestamp(corpus_source_filepath)\n    filepath = utility.path_add_suffix(filepath, '.tokenized')\n    \n    file_stats = []\n    process_count = 0\n    \n    # TODO: Enable store of all documents line-by-line in a single file\n    with zipfile.ZipFile(filepath, \"w\") as zf:\n        \n        for document_id, document_name, chunk_index, tokens in tokenized_docs: \n            \n            text = ' '.join([ t.replace(' ', '_') for t in tokens ])\n            store_name  = utility.path_add_sequence(document_name, chunk_index, 4)\n            \n            zf.writestr(store_name, text, zipfile.ZIP_DEFLATED)\n            \n            file_stats.append((document_id, document_name, chunk_index, len(tokens)))\n            \n            if process_count % 100 == 0:\n                logger.info('Stored {} files...'.format(process_count))\n                \n            process_count += 1\n            \n            \n    df_summary = pd.DataFrame(file_stats, columns=['document_id', 'document_name', 'chunk_index', 'n_tokens'])\n    \n    return filepath, df_summary\n\ndef display_generate_tokenized_corpus_gui(corpus, corpus_source_filepath, subst_filename=None):\n    \n    filenames = [ doc._.meta['filename'] for doc in corpus ]\n    \n    try:\n        document_index = domain_logic.compile_documents(corpus)\n    except Exception as ex:\n        document_index = domain_logic.compile_documents_by_filename(filenames)\n    \n    term_substitutions = { }\n    \n    if subst_filename is not None:\n        logger.info('Loading term substitution mappings...')\n        term_substitutions = textacy_utility.load_term_substitutions(subst_filename, default_term='_masked_', delim=';', vocab=corpus.spacy_lang.vocab)\n        \n    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n    pos_options = [('(All)', None)] + sorted([(k + ' (' + v + ')', k) for k,v in pos_tags.items() ])\n    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n    \n    lw = lambda width: widgets.Layout(width=width)\n    gui = types.SimpleNamespace(\n        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1, layout=widgets.Layout(width='400px')),\n        max_doc_freq=widgets.IntSlider(description='Min doc. %', min=75, max=100, value=100, step=1, layout=widgets.Layout(width='400px')),\n        substitute_terms=widgets.ToggleButton(value=False, description='Mask GPE',  tooltip='Replace geographical entites with `_gpe_`'),\n        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='180px')),\n        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n        chunk_size=widgets.Dropdown(description='Chunk size', options=[('None', 0), ('500', 500), ('1000', 1000), ('2000', 2000) ], value=0, layout=widgets.Layout(width='180px')),\n        normalize=widgets.Dropdown(description='Normalize', options=[ None, 'lemma', 'lower' ], value='lower', layout=widgets.Layout(width='180px')),\n        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords'),\n        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations'),\n        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities'),\n        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('100px')),\n        output=widgets.Output(layout={'border': '1px solid black'}),\n    )\n    \n    logger.info('Preparing corpus statistics...')\n    logger.info('...word counts...')\n    word_counts = { k: textacy_utility.generate_word_count_score(corpus, k, gui.min_freq.max) for k in [ 'lemma', 'lower', 'orth' ] }\n    \n    logger.info('...word document count...')\n    word_document_counts = { k: textacy_utility.generate_word_document_count_score(corpus, k, gui.max_doc_freq.min) for k in [ 'lemma', 'lower', 'orth' ] }\n\n    logger.info('...done!')\n    \n    gui.boxes = widgets.VBox([\n        gui.progress,\n        widgets.HBox([\n            widgets.VBox([\n                widgets.HBox([gui.normalize, gui.chunk_size]),\n                widgets.HBox([gui.ngrams, gui.min_word]),\n                gui.min_freq,\n                gui.max_doc_freq\n            ]),\n            widgets.VBox([\n                gui.include_pos\n            ]),\n            widgets.VBox([\n                gui.filter_stops,\n                gui.substitute_terms,\n                gui.filter_punct,\n                gui.named_entities\n            ]),\n            widgets.VBox([\n                gui.compute\n            ]),\n        ]),\n        gui.output\n    ])\n    \n    display(gui.boxes)\n    \n    def compute_callback(*_args):\n        gui.compute.disabled = True\n        filepath = ''\n        opts = dict(\n            min_freq=gui.min_freq.value,\n            max_doc_freq=gui.max_doc_freq.value,\n            substitute_terms=gui.substitute_terms.value,\n            ngrams=gui.ngrams.value,\n            min_word=gui.min_word.value,\n            normalize=gui.normalize.value,\n            filter_stops=gui.filter_stops.value,\n            filter_punct=gui.filter_punct.value,\n            named_entities=gui.named_entities.value,\n            include_pos=gui.include_pos.value,\n            chunk_size=gui.chunk_size.value,\n            term_substitutions=term_substitutions,\n            word_counts=word_counts,\n            word_document_counts=word_document_counts\n        )\n        \n        with gui.output:\n\n            docs = ((doc._.meta['filename'], doc) for doc in corpus)\n            \n            tokenized_docs = tokenize_docs(docs, **opts)\n            \n            filepath, df_summary = store_tokenized_corpus(tokenized_docs, corpus_source_filepath, **opts)\n            \n        gui.output.clear_output()\n        \n        with gui.output:\n            logger.info('Process DONE!')\n            logger.info(\"Result stored in '{}'\".format(filepath))\n            display(df_summary)\n            \n        gui.compute.disabled = False\n        \n    gui.compute.on_click(compute_callback)\n    \n    return gui\n\ntry:\n    subst_filename = os.path.join(domain_logic.DATA_FOLDER, 'term_substitutions.txt')\n    corpus = current_corpus_container().textacy_corpus\n    corpus_path =  current_corpus_container().prepped_source_path\n    #if not corpus is None :\n    #    docs = textacy_corpus_document_stream(corpus)\n    if corpus is None:\n        logger.info('Please load corpus!')\n    else:\n        display_generate_tokenized_corpus_gui(corpus, corpus_path)\nexcept Exception as ex:\n    raise\n    logger.error(ex)\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}
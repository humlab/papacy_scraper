{"cells":[{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## tCoIR - Text Analysis\n","### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Data folder: /home/roger/source/text_analytic_tools/data/tCoIR\n"}],"source":["\n","from beakerx.object import beakerx\n","from beakerx import *\n","\n","from IPython.display import display #, set_matplotlib_formats\n","import text_analytic_tools\n","import text_analytic_tools.utility as utility\n","import text_analytic_tools.common.textacy_utility as textacy_utility\n","import text_analytic_tools.common as common\n","\n","logger = utility.getLogger('tCoIR')\n","\n","current_domain = text_analytic_tools.CURRENT_DOMAIN\n","utility.setup_default_pd_display(pd)\n","container = None"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["### STEPS\n","\n","| Step | Input | Output | Note | Source |\n","|:---|:---|:---|:---|:---|\n","| 1. Select (create) text corpus | *.txt | xyz.txt.zip | Decide (specify) which text subset to use| (manual) |\n","| 2. Prepare text corpus | xyz.txt.zip | xyz.txt_preprocessed.zip | Minor preprocessing of text files (e.g. hyphens etc.) | textacy_utility.preprocess_text |\n","| 3. Load (or create) textaCy corpus | xyz.txt_preprocessed.zip | xyz.bin.bz2 | Loads textaCy corpus (creates if not exists) | load_or_create |\n","| 4. Create tokenized text corpus | xyz.bin.bz2  | xyz.tokenized.zip | Loads textaCy corpus (creates if not exists) | load_or_create |\n","| 5. **Compute co-occurrence** | xyz.tokenized.zip | (excel) | Compute co-occurrence | co_occurrence.compute |\n","\n","Note:\n","\n","- Steps 1 to 4 can be skipped if a tokenized corpus already exists.\n","- Steps 1 and 2 can be skipped if prepared corpus (or textaCy corpus) already exists\n","\n","Configuration elements:\n","\n","| Step | Configuration | Action |\n","|:---|:---|\n","| **1.** | What text files to include  |\n","| **2.** | (nothing) |\n","| **3.** | NER yes/no |\n","| **3.** | NER yes/no |\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["### OPTIONAL: Prepare a new filtered tokenized corpus.\n","\n","1. Load (or create a new) textaCy corpus from the source text files.\n","1. Create (extract) filtered tokenized corpus (to be used in co-occurrence)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"2019-10-27 20:35:34,736 : INFO : Loading term substitution mappings...\n2019-10-27 20:35:34,742 : INFO : Stored 0 files...\n2019-10-27 20:35:34,970 : INFO : Stored 100 files...\n2019-10-27 20:35:35,126 : INFO : Stored 200 files...\n2019-10-27 20:35:35,270 : INFO : Stored 300 files...\n2019-10-27 20:35:35,409 : INFO : Stored 400 files...\n2019-10-27 20:35:35,637 : INFO : Done! Result stored in '/home/roger/source/text_analytic_tools/data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.ADJ.NOUN.VERB.tokenized.zip'\n2019-10-27 20:35:35,640 : INFO : Stored 0 files...\n2019-10-27 20:35:35,866 : INFO : Stored 100 files...\n2019-10-27 20:35:36,011 : INFO : Stored 200 files...\n2019-10-27 20:35:36,143 : INFO : Stored 300 files...\n2019-10-27 20:35:36,273 : INFO : Stored 400 files...\n2019-10-27 20:35:36,418 : INFO : Done! Result stored in '/home/roger/source/text_analytic_tools/data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.ADJ.NOUN.tokenized.zip'\n2019-10-27 20:35:36,420 : INFO : Stored 0 files...\n2019-10-27 20:35:36,614 : INFO : Stored 100 files...\n2019-10-27 20:35:36,744 : INFO : Stored 200 files...\n2019-10-27 20:35:36,859 : INFO : Stored 300 files...\n2019-10-27 20:35:36,970 : INFO : Stored 400 files...\n2019-10-27 20:35:37,096 : INFO : Done! Result stored in '/home/roger/source/text_analytic_tools/data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.N.O.U.N.tokenized.zip'\n2019-10-27 20:35:37,099 : INFO : Stored 0 files...\n2019-10-27 20:35:37,240 : INFO : Stored 100 files...\n2019-10-27 20:35:37,330 : INFO : Stored 200 files...\n2019-10-27 20:35:37,412 : INFO : Stored 300 files...\n2019-10-27 20:35:37,493 : INFO : Stored 400 files...\n2019-10-27 20:35:37,591 : INFO : Done! Result stored in '/home/roger/source/text_analytic_tools/data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.V.E.R.B.tokenized.zip'\n"}],"source":["import text_analytic_tools.domain.common_logic as common_logic\n","\n","source_path = '/home/roger/source/text_analytic_tools/data/tCoIR/tCoIR_en_45-72.txt.zip'\n","\n","if container is None:\n","    container = textacy_utility.load_or_create(\n","        source_path=source_path,\n","        language='en',\n","        document_index=None,\n","        merge_entities=False,\n","        overwrite=False,\n","        use_compression=True,\n","        disabled_pipes=tuple((\"ner\", \"parser\", \"textcat\"))\n","    )\n","\n","corpus             = container.textacy_corpus\n","min_freq_stats     = { k: textacy_utility.generate_word_count_score(corpus, k, 10) for k in [ 'lemma', 'lower', 'orth' ] }\n","max_doc_freq_stats = { k: textacy_utility.generate_word_document_count_score(corpus, k, 75) for k in [ 'lemma', 'lower', 'orth' ] }\n","document_index     = common_logic.document_index(corpus)\n","term_substitutions = common_logic.term_substitutions(vocab=None)\n","fx_docs            = lambda corpus: ((doc._.meta['filename'], doc) for doc in corpus)\n","\n","default_opts = dict(\n","    term_substitutions=term_substitutions,\n","    substitute_terms=True,\n","    ngrams=[1],\n","    min_word=1,\n","    normalize='lemma',\n","    filter_stops=True,\n","    filter_punct=True,\n","    named_entities=False,\n","    include_pos=('ADJ', 'NOUN'),\n","    chunk_size=0,\n","    min_freq=1,\n","    min_freq_stats=min_freq_stats,                 # Must be specified if min_freq > 1\n","    max_doc_freq=100,\n","    max_doc_freq_stats=max_doc_freq_stats          # Must be specified if max_doc_freq < 100\n",")\n","\n","run_opts = [\n","    dict(include_pos=('ADJ', 'NOUN', 'VERB')),\n","    dict(include_pos=('ADJ', 'NOUN')),\n","    dict(include_pos=('NOUN')),\n","    dict(include_pos=('VERB'))\n","]\n","\n","for _opts in run_opts:\n","\n","    opts = utility.extend(default_opts, _opts)\n","\n","    target_filename = utility.path_add_timestamp(container.prepped_source_path)\n","    target_filename = utility.path_add_suffix(target_filename, '.' + opts.get('normalize',''))\n","    target_filename = utility.path_add_suffix(target_filename, '.' + '.'.join(list(opts.get('include_pos',''))))\n","    target_filename = utility.path_add_suffix(target_filename, '.tokenized')\n","\n","    tokenized_docs = textacy_utility.extract_document_tokens(fx_docs(corpus), **opts)\n","\n","    df_summary = common.store_tokenized_corpus_as_archive(tokenized_docs, target_filename)\n","\n","    logger.info(\"Done! Result stored in '{}'\".format(target_filename))\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"2019-10-28 07:40:32,070 : INFO : Initializing dictionary\n2019-10-28 07:40:32,075 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n2019-10-28 07:40:32,624 : INFO : built Dictionary(3729 unique tokens: ['above', 'accordance', 'art', 'article', 'artistic']...) from 483 documents (total 158609 corpus positions)\n2019-10-28 07:40:32,699 : INFO : Initializing dictionary\n2019-10-28 07:40:32,706 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n2019-10-28 07:40:33,282 : INFO : built Dictionary(3729 unique tokens: ['above', 'accordance', 'art', 'article', 'artistic']...) from 483 documents (total 158609 corpus positions)\n2019-10-28 07:40:33,745 : INFO : Builiding vocabulary...\n2019-10-28 07:40:33,766 : INFO : Vocabulary of size 3660 built.\n2019-10-28 07:40:33,773 : INFO : Year 1945...\n2019-10-28 07:40:34,495 : INFO : Year 1946...\n2019-10-28 07:40:35,565 : INFO : Year 1947...\n2019-10-28 07:40:37,105 : INFO : Year 1948...\n2019-10-28 07:40:39,382 : INFO : Year 1949...\n2019-10-28 07:40:40,218 : INFO : Year 1950...\n2019-10-28 07:40:41,187 : INFO : Year 1951...\n2019-10-28 07:40:42,203 : INFO : Year 1952...\n2019-10-28 07:40:43,250 : INFO : Year 1953...\n2019-10-28 07:40:45,527 : INFO : Year 1954...\n2019-10-28 07:40:46,944 : INFO : Year 1955...\n2019-10-28 07:40:47,743 : INFO : Year 1956...\n2019-10-28 07:40:49,546 : INFO : Year 1957...\n2019-10-28 07:40:51,497 : INFO : Year 1958...\n2019-10-28 07:40:52,907 : INFO : Year 1959...\n2019-10-28 07:40:54,590 : INFO : Year 1960...\n2019-10-28 07:40:56,415 : INFO : Year 1961...\n2019-10-28 07:40:58,445 : INFO : Year 1962...\n2019-10-28 07:40:59,653 : INFO : Year 1963...\n2019-10-28 07:41:01,097 : INFO : Year 1964...\n2019-10-28 07:41:02,926 : INFO : Year 1965...\n2019-10-28 07:41:04,502 : INFO : Year 1966...\n2019-10-28 07:41:06,466 : INFO : Year 1967...\n2019-10-28 07:41:08,223 : INFO : Year 1968...\n2019-10-28 07:41:09,714 : INFO : Year 1969...\n2019-10-28 07:41:11,719 : INFO : Year 1970...\n2019-10-28 07:41:13,303 : INFO : Year 1971...\n2019-10-28 07:41:14,810 : INFO : Year 1972...\nResult saved to file HAL_5_result_co_occurrence_20191028_0741.xlsx\n2019-10-28 07:43:05,515 : INFO : Builiding vocabulary...\n2019-10-28 07:43:05,534 : INFO : Vocabulary of size 3660 built.\n2019-10-28 07:43:05,539 : INFO : Year 1945...\n2019-10-28 07:43:06,081 : INFO : Year 1946...\n2019-10-28 07:43:07,095 : INFO : Year 1947...\n2019-10-28 07:43:08,533 : INFO : Year 1948...\n2019-10-28 07:43:10,647 : INFO : Year 1949...\n2019-10-28 07:43:11,514 : INFO : Year 1950...\n2019-10-28 07:43:14,321 : INFO : Year 1951...\n2019-10-28 07:43:15,334 : INFO : Year 1952...\n2019-10-28 07:43:16,270 : INFO : Year 1953...\n2019-10-28 07:43:17,844 : INFO : Year 1954...\n2019-10-28 07:43:19,088 : INFO : Year 1955...\n2019-10-28 07:43:19,798 : INFO : Year 1956...\n2019-10-28 07:43:21,539 : INFO : Year 1957...\n2019-10-28 07:43:23,505 : INFO : Year 1958...\n2019-10-28 07:43:24,876 : INFO : Year 1959...\n2019-10-28 07:43:26,535 : INFO : Year 1960...\n2019-10-28 07:43:28,360 : INFO : Year 1961...\n2019-10-28 07:43:30,347 : INFO : Year 1962...\n2019-10-28 07:43:31,840 : INFO : Year 1963...\n2019-10-28 07:43:33,203 : INFO : Year 1964...\n2019-10-28 07:43:34,684 : INFO : Year 1965...\n2019-10-28 07:43:36,201 : INFO : Year 1966...\n2019-10-28 07:43:38,147 : INFO : Year 1967...\n2019-10-28 07:43:39,939 : INFO : Year 1968...\n2019-10-28 07:43:41,399 : INFO : Year 1969...\n2019-10-28 07:43:43,479 : INFO : Year 1970...\n2019-10-28 07:43:45,073 : INFO : Year 1971...\n2019-10-28 07:43:46,509 : INFO : Year 1972...\n"},{"ename":"NameError","evalue":"name 'method' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-5b9a3614efa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco_occurrence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'HAL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mresult_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CO_tCoIR_en_45-72.{}_{}_{}_{}.xlsx'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d_%H%M\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'method' is not defined"]}],"source":["import time\n","import text_analytic_tools.text_analysis.co_occurrence as co_occurrence\n","import text_analytic_tools.common.text_corpus as text_corpus\n","\n","def compute_co_occurrence(filepath, window_size=5, distance_metric=0, method='HAL', normalize='size'):\n","\n","    corpus = text_corpus.SimplePreparedTextCorpus(filepath, lowercase=True)\n","    document_index = current_domain.compile_documents(corpus)\n","\n","    df = co_occurrence.compute(corpus, document_index, window_size, distance_metric, normalize, method)\n","\n","    result_filename = '{}_{}_result_co_occurrence_{}.xlsx'.format(method, window_size, time.strftime(\"%Y%m%d_%H%M\"))\n","    df.to_excel(result_filename)\n","    \n","    print('Result saved to file {}'.format(result_filename))\n","\n","source_files = [\n","    ('data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.ADJ.NOUN.tokenized.zip', 'lemma.ADJ.NOUN'),\n","    ('data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.ADJ.NOUN.VERB.tokenized.zip', 'lemma.ADJ.NOUN.VERB'),\n","    ('data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.NOUN.tokenized.zip', 'lemma.NOUN'),\n","    ('data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.VERB.tokenized.zip', 'lemma.VERB')\n","]\n","\n","for source_file, tag in source_files:\n","    corpus = text_corpus.SimplePreparedTextCorpus(source_file, lowercase=True)\n","    document_index = current_domain.compile_documents(corpus)\n","    for window_size in [5, 10, 20]:\n","        compute_co_occurrence(source_file, window_size=5, distance_metric=0, method='HAL')\n","        df = co_occurrence.compute(corpus, document_index, window_size=5, distance_metric=0, normalize='size', method='HAL')\n","\n","        result_filename = 'CO_tCoIR_en_45-72.{}_{}_{}_{}.xlsx'.format(time.strftime(\"%Y%m%d_%H%M\", method, window_size, tag))\n","        df.to_excel(result_filename)\n","        \n","        print('Result saved to file {}'.format(result_filename))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["!mv data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.V.E.R.B.tokenized.zip data/tCoIR/tCoIR_en_45-72.txt_preprocessed_201910272035.lemma.VERB.tokenized.zip"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["https://github.com/maciejkula/glove-python/issues/96\n","\n","```bash\n","% git clone https://github.com/maciejkula/glove-python.git\n","% cd glove-python/\n","% cd glove/\n","% cython glove_cython.pyx\n","% cythonize glove_cython.pyx\n","% cython metrics/accuracy_cython.pyx\n","% cythonize metrics/accuracy_cython.pyx\n","% cython --cplus corpus_cython.pyx\n","% cythonize corpus_cython.pyx\n","% cd ..\n","% python setup.py cythonize\n","% make\n","% pip install -e .\n","```\n","\n","  "]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}
{"cells":[{"cell_type":"markdown","metadata":{},"source":["## The Culture of International Relations - Text Analysis\n","### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"]},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"code_folding":[]},"outputs":[],"source":"# Setup\n%load_ext autoreload\n%autoreload 2\n\nimport sys, os\nimport nltk, textacy, spacy \nimport pandas as pd\nimport ipywidgets\nimport bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\nimport text_analytic_tools.utility as utility\nimport text_analytic_tools.utility.widgets as widgets\nimport text_analytic_tools.common.text_corpus as text_corpus\nimport text_analytic_tools.common.textacy_utility as textacy_utility\nimport warnings\n\nfrom beakerx.object import beakerx\nfrom beakerx import *\nfrom IPython.display import display, set_matplotlib_formats\n\nlogger = utility.getLogger('corpus_text_analysis')\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\nfrom text_analytic_tools.domain_logic_config import current_domain as domain_logic\n\nutility.setup_default_pd_display(pd)\n\nDATA_FOLDER, PATTERN = '../../data',  '*.txt'\nDF_TAGSET = pd.read_csv(os.path.join(DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n\n%matplotlib inline\n\n# set_matplotlib_formats('svg')   \nbokeh.plotting.output_notebook()\n\ncurrent_corpus_container = lambda: textacy_utility.CorpusContainer.container()\ncurrent_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n"},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import text_analytic_tools.notebooks_gui.textacy_corpus_gui\n\ntry:\n    container = current_corpus_container()\n    # FIXME VARYING ASPECTS: document_index = WTI_INDEX for tCoIR\n    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, document_index=None, container=container, compute_ner=True, domain_logic=domain_logic)\nexcept Exception as ex:\n    raise\n    logger.error(ex)"},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>TRY IT</span>\n","Spacy NER, note that \"ner\" must be enabled in corpus pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"code_folding":[0]},"outputs":[],"source":"# Display Named Entities\nfrom spacy import displacy\n\ndef display_document_entities_gui(corpus, document_index):\n    \n    # FIXME VARYING ASPECT: Add \"document_name\" to document_index, or function that creates name\n    filenames = document_index.filename\n    document_options = list(sorted(zip(filenames,filenames.index), key=lambda x: x[0]))\n\n    gui = types.SimpleNamespace(\n        position=1,\n        output=widgets.Output(layout={'border': '1px solid black'}),\n        document_id=widgets.Dropdown(description='Document', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='50%')),\n        left=widgets.Button(description='<<', button_style='Success', layout=widgets.Layout(width='40px')),\n        right=widgets.Button(description='>>', button_style='Success', layout=widgets.Layout(width='40px')),\n    )\n\n    def display_document_entities(corpus, document_id):\n        gui.output.clear_output()\n        with gui.output:        \n            doc = textacy_utility.get_document_by_id(corpus, document_id)\n            displacy.render(doc, style='ent', jupyter=True)\n    \n    def back_handler(*args):\n        if gui.position == 0:\n            return\n        gui.output.clear_output()\n        gui.position = (gui.position - 1) % len(document_options)\n        gui.document_id.value = document_options[gui.position][1]\n        #itw.update()\n        \n    def forward_handler(*args):\n        gui.output.clear_output()\n        gui.position = (gui.position + 1) % len(document_options)\n        gui.document_id.value = document_options[gui.position][1]\n    \n    gui.left.on_click(back_handler)\n    gui.right.on_click(forward_handler)\n    \n    display(widgets.VBox([\n        widgets.HBox([gui.document_id, gui.left, gui.right]),\n        widgets.VBox([gui.output], layout=widgets.Layout(margin_top='20px', height='600px',width='100%'))\n    ]))\n    \n    itw = widgets.interactive(\n        display_document_entities,\n        corpus=widgets.fixed(corpus),\n        document_id=gui.document_id\n    )\n    \n    itw.update()\n    \ntry:\n    corpus = current_corpus()\n    document_index = domain_logic.compile_documents(corpus)\n    display_document_entities_gui(corpus, document_index=document_index)\nexcept Exception as ex:\n    logger.error(ex)\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def extract_entities(doc, include_types=None, drop_determiners=True):\n    \n    entities = (x for x in doc.ents if not x.text.isspace())\n    \n    if include_types is not None:\n        assert isinstance(include_types, (set, list, tuple))\n        entities = (x for x in entities if x.label_ in include_types)\n\n    if drop_determiners is True:\n        entities = (x if x[0].pos != DET else SpacySpan(x.doc, x.start + 1, x.end, label=x.label, vector=x.vector) for x in entities)\n\n    for x in entities:\n        yield x"},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green;'>MODEL</span> Extract Named Entities<span style='color: green; float: right'>TRY IT</span>\n","Spacy NER, note that \"ner\" must be enabled in corpus pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"%%bash\nnohup python3 run_ner_places.py &\n"},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green;'>DESCRIBE</span> Display Named Entity Statistics<span style='color: green; float: right'>TRY IT</span>\n","Spacy NER, note that \"ner\" must be enabled in corpus pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"\ndef compile_named_entity_data(corpus, document_index, drop_determiners=True, min_freq=1):\n    #textacy.extract.entities(doc, include_types=None, exclude_types=None, drop_determiners=True, min_freq=1)\n    data = [[\n        (doc._.meta['document_id'], ent[0].ent_type_, ent.text, ent.lemma_)\n             for ent in textacy.extract.entities(doc, exclude_types=('CARDINAL',), drop_determiners=drop_determiners, min_freq=min_freq) ]\n                for doc in corpus\n    ]\n    data = utility.flatten(data)\n    df = pd.DataFrame(data, columns=['document_id', 'ent_type', 'text', 'lemma']).set_index('document_id')\n    df = pd.merge(df, document_index, left_index=True, right_index=True, how='inner')\n    return df[df.year > 0][['pope', 'year', 'genre', 'ent_type', 'text', 'lemma', 'filename']].reset_index()\n\ndef display_grouped_by_entities_gui(corpus, document_index):\n    \n    columns = compile_named_entity_data([corpus[0]], document_index).columns\n    \n    group_by_options = [ (x.title(), x) for x in columns if x not in [ 'ent_type', 'text', 'lemma', 'filename', 'index'] ]\n    group_by_values = [ x for _, x in group_by_options ]\n    gui = types.SimpleNamespace(\n        group_by=widgets.SelectMultiple(description='Group by', options=group_by_options, value=group_by_values, rows=3, layout=widgets.Layout(width='180px')),\n        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n        drop_determiners=widgets.ToggleButton(value=True, description='Drop DET',  tooltip='Drop_determiners`', icon='check'),\n        output=widgets.Output(layout={'border': '1px solid black'}),\n        min_freq=widgets.IntSlider(description='Min freq', min=1, max=10, value=2, step=1, layout=widgets.Layout(width='400px')),\n    )\n        \n    def display_grouped_by_entities(corpus, group_by, drop_determiners, min_freq):\n        gui.output.clear_output()\n        named_entities = compile_named_entity_data(corpus, document_index, drop_determiners, min_freq)\n        with gui.output:\n            df = named_entities.groupby(list(group_by) + ['ent_type', 'lemma']).size().reset_index()\n            df = df.rename(columns={0:'Count'})\n            df = df.sort_values('Count', ascending=False)\n            display(df)\n\n    itw = widgets.interactive(\n        display_grouped_by_entities,\n        corpus=widgets.fixed(corpus),\n        #named_entities=widgets.fixed(named_entities),\n        group_by=gui.group_by,\n        drop_determiners=gui.drop_determiners,\n        min_freq=gui.min_freq\n    )\n\n    display(widgets.VBox([\n        widgets.HBox([gui.group_by, gui.drop_determiners, gui.min_freq]),\n        widgets.VBox([gui.output]),\n        itw.children[-1]\n    ]))\n    \n    \ntry:\n    corpus = current_corpus()\n    document_index = domain_logic.compile_documents(corpus)\n    display_grouped_by_entities_gui(corpus, document_index)\nexcept Exception as ex:\n    raise\n    logger.error(ex)\n"},{"cell_type":"markdown","metadata":{},"source":["## <span style='color: green;'>MODEL</span> Stanford NER Tagger (CoreNLP)<span style='color: green; float: right'>SKIP</span>"]},{"cell_type":"markdown","metadata":{},"source":["### <span style='color: green;'>PREPARE</span> Verify that Stanford CoreNLP is up and running<span style='color: green; float: right'>SKIP</span>\n","Stanford CoreNLP server must be started as described in:  https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n","\n","With docker:\n","```bash\n","docker pull frnkenstien/corenlp\n","docker run -p 9000:9000 --name coreNLP --rm -i -t frnkenstien/corenlp\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"STANFORD_CORE_NLP_URL = 'http://localhost:9000'\n\ntry:\n    from nltk.parse import corenlp\n    corenlp_tagger = corenlp.CoreNLPParser(url=STANFORD_CORE_NLP_URL, encoding='utf8', tagtype='ner')\n    input_tokens = 'Stony Brook University in NY'.split()\n    tagged_output = corenlp_tagger.tag(input_tokens)\n    print('Stanford tagger is up and running!')\n    print(' Result: ' + ' '.join([ x + '/' + y for x,y in tagged_output]))\nexcept: # (ConnectionError, ConnectionRefusedError):\n    logger.error('Server not found! Please start Stanford CoreNLP Server!')\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def load_document_index(path):\n    reader = text_corpus.CompressedFileReader(path)\n    df = domain_logic.compile_documents_by_filename(reader.filenames)\n    df['document_id'] = df.document_id.astype(np.int32)\n    df = df.rename(columns={'document_id': 'doc_id'})\n    df = df.set_index('doc_id')\n    df = df[['pope', 'genre', 'year', 'filename']]\n    return df\n\ndef load_ner_result(path):\n    df = pd.read_csv(path, sep='\\t')\n    df['doc_id'] = df.doc_id.astype(np.int32)\n    df = df.set_index('id')\n    return df\n\ndef compile_ner_result(source_path, result_path):\n    document_index = load_document_index(source_path)\n    df_ner = load_ner_result(result_path)\n    df_agg = df_ner.groupby(['doc_id', 'ent_type', 'entity']).size().reset_index().rename(columns={0: 'entity_count'})\n    df_doc_agg = document_index.merge(df_agg, left_index=True, right_on='doc_id', how='inner')\n    return df_doc_agg\n\ndf_francis = compile_ner_result('../../data/benedict-xvi_curated_20190326.txt.zip', 'ner_benedict-xvi_curated_20190326.txt_20190319193045.txt')\ndf_benedict = compile_ner_result('../../data/francis_curated_20190326.txt.zip', 'ner_francis_curated_20190326.txt_20190326145420.txt')\n\ndf_popes = pd.concat([df_francis, df_benedict], ignore_index=True, axis=0)\n\ndf_popes_entity_agg = df_popes.groupby(['pope', 'entity']).agg({'entity_count': 'sum'}).reset_index()\ndf_entity_agg = df_popes.groupby(['entity']).agg({'entity_count': 'sum'}).reset_index()\n\nprint(df_popes_entity_agg.entity_count.sum(), df_entity_agg.entity_count.sum())\n    \nwith pd.ExcelWriter('ner_francis_and_benedict_stanford_20190326.xlsx') as writer:\n    df_popes.to_excel(writer, sheet_name='Pivot (document)'),\n    df_popes_entity_agg.to_excel(writer, sheet_name='Pivot (pope, entity)'),\n    df_entity_agg.to_excel(writer, sheet_name='Pivot (entity)'),\n    \n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# df_ner[df_ner.doc_id == 3]\ndf_agg = df_ner.groupby(['doc_id', 'ent_type', 'entity']).size().reset_index().rename(columns={0: 'count'})\ndf_agg\n\ndf_pope_year_genre_type_agg = df_doc_agg.groupby(['pope', 'year', 'genre', 'ent_type', 'entity']).agg({'count': 'sum'}).reset_index()\n\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}
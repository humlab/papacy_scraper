{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> UNESCO Courier Sample Corpus <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'en_core_web_sm'\n",
    "import domain_logic_unesco as domain_logic\n",
    "\n",
    "# Streaming spaCy docs...\n",
    "def create_nlp(model='en_core_web_sm', disable=None):\n",
    "    '''Create a spaCy nlp model'''\n",
    "    nlp = spacy.load(model, disable=disable)\n",
    "    nlp.tokenizer = textacy_utility.keep_hyphen_tokenizer(nlp)\n",
    "    return nlp\n",
    "\n",
    "def create_source_stream(source_path, lang, document_index=None):\n",
    "    '''Returns a filename/text stream from txt files found in given zip file '''\n",
    "    reader = text_corpus.CompressedFileReader(source_path)\n",
    "    stream = domain_logic.get_document_stream(reader, lang, document_index=document_index)\n",
    "    return stream\n",
    "\n",
    "def source_document_stream(source_path, document_index=None, model='en_core_web_sm', files_of_interest=None):\n",
    "    '''Returns a filename/spaCy doc stream for text files in given zip file'''\n",
    "    logger.info('Loading model {}...'.format(model))\n",
    "    nlp = create_nlp(model=model, disable=('ner', 'parser', 'textcat'))\n",
    "    logger.info('Processing {}...'.format(source_path))\n",
    "    stream = create_source_stream(source_path, model[:2])\n",
    "    file_counter = 0\n",
    "    \n",
    "    for filename, text, _ in stream:\n",
    "        if files_of_interest is not None:\n",
    "            if not filename in files_of_interest:\n",
    "                continue\n",
    "                    \n",
    "        file_counter += 1\n",
    "        doc = nlp(text)\n",
    "        #doc['tensor'] = None\n",
    "        if file_counter % 10 == 0:\n",
    "            logger.info('Processed {} files...'.format(file_counter))\n",
    "            break\n",
    "        yield filename, doc\n",
    "        doc = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import text_corpus\n",
    "import re, os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import text_corpus\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger('UNESCO')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# FIXME VARYING ASPECTS: \n",
    "DOCUMENT_FILTERS = [\n",
    "    ]\n",
    "        \n",
    "GROUP_BY_OPTIONS = [\n",
    "    ('Year', ['year']),\n",
    "]\n",
    "\n",
    "FILE_PATTERN = r'([0-9]+)(\\w{2})\\w*\\.txt'\n",
    "\n",
    "def split_name(filename):\n",
    "    m = re.match(FILE_PATTERN, filename)\n",
    "    if m is None:\n",
    "        logger.error('Parse failed for filename: ' + filename)\n",
    "        return None, None\n",
    "    g = m.groups()\n",
    "    return g[0], g[1]\n",
    "\n",
    "def compile_documents_by_filename(filenames):\n",
    "\n",
    "    local_numbers, langs = list(zip(*[ split_name(x) for x in filenames ]))\n",
    "\n",
    "    df = pd.DataFrame( {\n",
    "        'local_number': [ int(x) for x in local_numbers],\n",
    "        'document_id': [ int(x) for x in local_numbers],\n",
    "        'filename': filenames,\n",
    "        'lang': langs,\n",
    "        'year': 0\n",
    "    })\n",
    "    #df = df.set_index('local_number')\n",
    "    \n",
    "    df['title'] = df.document_id.apply(lambda x: str(x).zfill(10))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def chunk_text(text, step=50000):\n",
    "    for segment in ( text[i:i + step] for i in range(0,len(text), step) ):\n",
    "        yield segment\n",
    "\n",
    "def load_corpus_index(corpus_name):\n",
    "    '''Given a corpus filename \"xxxxx.zip\", the document index name is expected to be \"xxxxx_index.txt\"'''\n",
    "    \n",
    "    basename, _ = os.path.splitext(corpus_name)\n",
    "    corpus_index_name = '{}_index.txt'.format(basename)\n",
    "    global_index_name = './global_corpus_index.txt'    \n",
    "    \n",
    "    if not (os.path.isfile(corpus_index_name) or os.path.isfile(global_index_name)):\n",
    "        logger.info('No Corpus Index found (looked for {} or {}.)'.format(corpus_index_name, global_index_name))\n",
    "        return None\n",
    "    \n",
    "    index_name = corpus_index_name if os.path.isfile(corpus_index_name) else global_index_name\n",
    "    \n",
    "    logger.info('Using corpus index: '.format(index_name))\n",
    "    \n",
    "    df = pd.read_csv(index_name, sep='\\t')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def compile_unesco_corpus_index(source):\n",
    "\n",
    "    df = None\n",
    "    if hasattr(source, 'path') or isinstance(source, str):\n",
    "        # Try to load pre-compiled index if exists\n",
    "        source_path = source.path if hasattr(source, 'path') else source\n",
    "        df = load_corpus_index(source_path)\n",
    "        if df is not None:\n",
    "            df['local_number'] = df.local_number.astype(np.int64)\n",
    "            df = df.set_index('local_number')\n",
    "            df['local_number'] = df.index\n",
    "            df['document_id'] = df.index\n",
    "            return df\n",
    "\n",
    "    if hasattr(source, 'filenames'):\n",
    "        # Fallback, cretae index out of file names\n",
    "        df = compile_documents_by_filename(source.filenames).set_index('local_number')\n",
    "        df['local_number'] = df.index\n",
    "        if 'year' not in df.columns:\n",
    "            df['year'] = 0\n",
    "        return df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def compile_documents(corpus, index=None):\n",
    "    \n",
    "    filenames = [ x._.meta['filename'] for x in corpus ]\n",
    "    \n",
    "    df = compile_documents_by_filename(filenames)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_document_stream(source, lang, **kwargs):\n",
    "\n",
    "    reader = text_corpus.CompressedFileReader(source) if isinstance(source, str) else source\n",
    "    \n",
    "    df_corpus_index = compile_unesco_corpus_index(reader)\n",
    "    \n",
    "    reader.filenames = sorted(list(df_corpus_index[(df_corpus_index.year//10).isin([196, 197])].filename.values))\n",
    "    \n",
    "    df_corpus_index = df_corpus_index.loc[df_corpus_index.filename.isin(reader.filenames)].sort_values('filename')\n",
    "    \n",
    "    assert len(reader.filenames) == len(df_corpus_index)\n",
    "    \n",
    "    row_id = 0\n",
    "    for filename, text in reader:\n",
    "        \n",
    "        local_number, lang = split_name(filename)\n",
    "        local_number = int(local_number)\n",
    "        \n",
    "        metadata = df_corpus_index.loc[local_number].to_dict()\n",
    "        \n",
    "        if metadata['n_words'] < 50:\n",
    "            logger.info('WARNING: Skipping empty file {} '.format(filename))\n",
    "            continue\n",
    "            \n",
    "        if metadata['lang'] != 'en':\n",
    "            logger.info('WARNING: Skipping file (unknown language) {} '.format(filename))\n",
    "            continue\n",
    "        \n",
    "        trunc_size = 50000\n",
    "        if metadata['n_bytes'] > trunc_size:\n",
    "            logger.info('WARNING: Truncating huge file {} '.format(filename))\n",
    "            text = text[:trunc_size]\n",
    "            \n",
    "#        i = 0\n",
    "#        for segment in chunk_text(text, step=50000):\n",
    "#            basename = str(local_number).zfill(10) + '_' + str(i).zfill(3)\n",
    "#            yield filename, text[:100000], metadata\n",
    "#            i += 1\n",
    "            \n",
    "        yield filename, text, metadata\n",
    "        row_id += 1\n",
    "\n",
    "# FIXME VARYING ASPECTs: What attributes to extend\n",
    "def add_domain_attributes(df, document_index):\n",
    "    df_extended = pd.merge(df, document_index, left_index=True, right_index=True, how='inner')    \n",
    "    return df_extended[['filename', 'year']]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = './../../data/unesco_test_corpus_20190307.txt_preprocessed.zip'\n",
    "reader = text_corpus.CompressedFileReader(source_path)\n",
    "df_corpus_index = compile_unesco_corpus_index(reader)\n",
    "\n",
    "reader = text_corpus.CompressedFileReader(source_path)\n",
    "\n",
    "df_corpus_index = compile_unesco_corpus_index(reader)\n",
    "\n",
    "reader.filenames = sorted(list(df_corpus_index[(df_corpus_index.year//10).isin([196, 197])].filename.values))\n",
    "\n",
    "df_corpus_index = df_corpus_index.loc[df_corpus_index.filename.isin(reader.filenames)].sort_values('filename')\n",
    "\n",
    "assert len(reader.filenames) == len(df_corpus_index)\n",
    "\n",
    "#len(df_corpus_index[(df_corpus_index.year>=1960)&(df_corpus_index.year<1970)].filename.values)\n",
    "len(df_corpus_index[(df_corpus_index.year//10).isin([196, 197])].filename.values)\n",
    "#filenames = df_corpus_index.loc[((df_corpus_index.year >= 1960 & df_corpus_index.year >= 1960))].valuesprint('hej')\n",
    "stream = get_document_stream(source_path, 'en')\n",
    "\n",
    "for x in stream:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    row_id = 0\n",
    "    for filename, text in reader:\n",
    "        \n",
    "        local_number, lang = split_name(filename)\n",
    "        local_number = int(local_number)\n",
    "        namedata = lookup.loc[filename].to_dict()\n",
    "        metadata = df_index.loc[local_number].to_dict()\n",
    "        metadata.update(namedata)\n",
    "        \n",
    "        if not (1960 <= metadata['year'] < 1970):\n",
    "            #print('SKIPPING: file since not 1960ths ' + filename)\n",
    "            continue\n",
    "            \n",
    "        if metadata['n_words'] < 50:\n",
    "            print('WARNING: skipping empty file ' + filename)\n",
    "            continue\n",
    "            \n",
    "        if metadata['lang'] != 'en':\n",
    "            print('WARNING: Unknown language, skipping file ' + filename)\n",
    "            continue\n",
    "        \n",
    "        trunc_size = 50000\n",
    "        if metadata['n_bytes'] > trunc_size:\n",
    "            print('WARNING: Truncating file ' + filename)\n",
    "            text = text[:trunc_size]\n",
    "            \n",
    "#        i = 0\n",
    "#        for segment in chunk_text(text, step=50000):\n",
    "#            basename = str(local_number).zfill(10) + '_' + str(i).zfill(3)\n",
    "#            yield filename, text[:100000], metadata\n",
    "#            i += 1\n",
    "            \n",
    "        row_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Find Key Terms - Semantic Network<span style='float: right; color: green'>WORK IN PROGRESS</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc\n",
    "normalize='lemma'\n",
    "window_width=2\n",
    "edge_weighting='binary'\n",
    "ranking_algo='pagerank'\n",
    "join_key_words=False,\n",
    "n_keyterms=10\n",
    "\n",
    "\n",
    "if isinstance(n_keyterms, float):\n",
    "    if not 0.0 < n_keyterms <= 1.0:\n",
    "        raise ValueError('`n_keyterms` must be an int, or a float between 0.0 and 1.0')\n",
    "    n_keyterms = int(round(len(doc) * n_keyterms))\n",
    "\n",
    "    \n",
    "include_pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "word_list = textacy.extract.words()\n",
    "if normalize == 'lemma':\n",
    "    word_list = [word.lemma_ for word in doc]\n",
    "    good_word_list = [word.lemma_ for word in doc\n",
    "                      if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "elif normalize == 'lower':\n",
    "    word_list = [word.lower_ for word in doc]\n",
    "    good_word_list = [word.lower_ for word in doc\n",
    "                      if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "elif not normalize:\n",
    "    word_list = [word.text for word in doc]\n",
    "    good_word_list = [word.text for word in doc\n",
    "                      if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "else:\n",
    "    word_list = [normalize(word) for word in doc]\n",
    "    good_word_list = [normalize(word) for word in doc\n",
    "                      if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "\n",
    "good_word_list = [word for word in good_word_list if word]\n",
    "\n",
    "graph = network.terms_to_semantic_network(good_word_list, window_width=window_width, edge_weighting=edge_weighting)\n",
    "\n",
    "# rank nodes by algorithm, and sort in descending order\n",
    "if ranking_algo == 'pagerank':\n",
    "    word_ranks = nx.pagerank_scipy(graph, weight='weight')\n",
    "elif ranking_algo == 'divrank':\n",
    "    word_ranks = rank_nodes_by_divrank(graph, r=None, lambda_=kwargs.get('lambda_', 0.5), alpha=kwargs.get('alpha', 0.5))\n",
    "elif ranking_algo == 'bestcoverage':\n",
    "    word_ranks = rank_nodes_by_bestcoverage(graph, k=n_keyterms, c=kwargs.get('c', 1), alpha=kwargs.get('alpha', 1.0))\n",
    "\n",
    "# bail out here if all we wanted was key *words* and not *terms*\n",
    "if join_key_words is False:\n",
    "    return [(word, score) for word, score in sorted(word_ranks.items(), key=operator.itemgetter(1), reverse=True)[:n_keyterms]]\n",
    "\n",
    "top_n = int(0.25 * len(word_ranks))\n",
    "top_word_ranks = { word: rank for word, rank in sorted(word_ranks.items(), key=operator.itemgetter(1), reverse=True)[:top_n] }\n",
    "\n",
    "# join consecutive key words into key terms\n",
    "seen_joined_key_terms = set()\n",
    "joined_key_terms = []\n",
    "for key, group in itertools.groupby(word_list, lambda word: word in top_word_ranks):\n",
    "    if key is True:\n",
    "        words = list(group)\n",
    "        term = ' '.join(words)\n",
    "        if term in seen_joined_key_terms:\n",
    "            continue\n",
    "        seen_joined_key_terms.add(term)\n",
    "        joined_key_terms.append((term, sum(word_ranks[word] for word in words)))\n",
    "\n",
    "return sorted(joined_key_terms, key=operator.itemgetter(1, 0), reverse=True)[:n_keyterms]\n",
    "\n",
    "\n",
    "corpus = current_corpus()\n",
    "doc = corpus[0]\n",
    "include_pos = { 'NOUN', 'PROPN', 'ADJ' }\n",
    "#exclude_pos = { 'PRON' }\n",
    "window_width = 10\n",
    "edge_weighting = 'cooc_freq'\n",
    "#key_terms = textacy.keyterms.key_terms_from_semantic_network(\n",
    "#    doc, normalize='lemma', window_width=4, edge_weighting='cooc_freq', ranking_algo='pagerank', join_key_words=False, n_keyterms=1000000\n",
    "#)\n",
    "\n",
    "terms = [x for x in textacy.extract.words(doc, filter_stops=True, filter_punct=True, filter_nums=False, include_pos=include_pos, exclude_pos=exclude_pos, min_freq=1)]\n",
    "graph = textacy.network.terms_to_semantic_network(terms, normalize='lemma', window_width=window_width, edge_weighting=edge_weighting)\n",
    "nodes = [{'name': str(i)} for i in graph.nodes(data=True)]\n",
    "links = [{'source': u[0], 'target': u[1], 'weight': u[2] } for u in graph.edges(data='weight')]\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(1, 1, figsize=(22, 12));\n",
    "nx.draw_networkx(graph, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glove\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 20)\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "def m2df(matrix, columns):\n",
    "    return pd.DataFrame(\n",
    "        data=matrix,\n",
    "        index=list(columns),\n",
    "        columns=list(columns)\n",
    "    )\n",
    "\n",
    "corpus_words = ['the', 'basic', 'concept', 'of', 'the', 'word', 'association']\n",
    "\n",
    "token2id = { }\n",
    "for w in corpus_words:\n",
    "    if w not in token2id:\n",
    "        token2id[w] = len(token2id)\n",
    "                            \n",
    "id2token = { k:w for w, k in token2id.items() }\n",
    "columns = [ id2token[i] for i in range(0, len(id2token))]\n",
    "\n",
    "print(token2id)\n",
    "print(id2token)\n",
    "\n",
    "#id2word = { k: w for w,k in dictionary.items() }\n",
    "\n",
    "corpus = [corpus_words, corpus_words]\n",
    "\n",
    "model = glove.Corpus(dictionary=dictionary)\n",
    "model.fit(corpus, window=5)\n",
    "\n",
    "X = model.matrix.astype(np.float64)\n",
    "\n",
    "Xi = X.sum(axis=1)\n",
    "Pij = (X / Xi)\n",
    "Pij[np.isnan(Pij)] = 0\n",
    "Pji = Pij.T\n",
    "\n",
    "print(m2df(model.matrix.todense(), columns))\n",
    "print(m2df(Pij, columns))\n",
    "print(m2df(Pji, columns))\n",
    "print(m2df(Pij+Pji, columns))\n",
    "\n",
    "Pij.sum(axis=1)\n",
    "\n",
    "#print(model.dictionary)\n",
    "\n",
    "#Pij = np.divide(X, Xi, out=np.zeros_like(X), where=Xi!=0)\n",
    "    \n",
    "#for word in corpus_words:\n",
    "#    assert word in model.dictionary\n",
    "\n",
    "#assert model.matrix.shape == (len(corpus_words), len(corpus_words))\n",
    "\n",
    "#expected = [[0.0, 1.0, 0.5],\n",
    "#            [0.0, 0.0, 1.0],\n",
    "#            [0.0, 0.0, 0.0]]\n",
    "\n",
    "#assert (model.matrix.todense().tolist() == expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "p(A, B, k) &= \\frac{cooccurrence(A, B, K)}{count(A)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = 'The Horse Raced Past The Barn Fell .'.lower().split()\n",
    "answer = {\n",
    " 'barn':  {'.': 4,  'barn': 0,  'fell': 5,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n",
    " 'fell':  {'.': 5,  'barn': 0,  'fell': 0,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n",
    " 'horse': {'.': 0,  'barn': 2,  'fell': 1,  'horse': 0,  'past': 4,  'raced': 5,  'the': 3},\n",
    " 'past':  {'.': 2,  'barn': 4,  'fell': 3,  'horse': 0,  'past': 0,  'raced': 0,  'the': 5},\n",
    " 'raced': {'.': 1,  'barn': 3,  'fell': 2,  'horse': 0,  'past': 5,  'raced': 0,  'the': 4},\n",
    " 'the':   {'.': 3,  'barn': 6,  'fell': 4,  'horse': 5,  'past': 3,  'raced': 4,  'the': 2}\n",
    "}\n",
    "df_answer = pd.DataFrame(answer)\n",
    "df_answer\n",
    "#list(sliding_window3(chain(terms, [None]*5), n=5))\n",
    "#list(sliding_window_it(terms, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def skipgrams(sequence, n, k):\n",
    "    \"\"\"Modified from NLTK to give distance.\"\"\"\n",
    "    \n",
    "    SENTINEL = None #object()\n",
    "    for ngram in ngrams(sequence, n + k, pad_right=True, right_pad_symbol=SENTINEL):\n",
    "        head = ngram[:1]\n",
    "        tail = ngram[1:]\n",
    "        #print(ngram, head, tail)\n",
    "        for i, skip_tail in enumerate(combinations(tail, n - 1)):\n",
    "            if skip_tail[-1] is SENTINEL:\n",
    "                continue\n",
    "            #print(head + skip_tail, k-i)\n",
    "            yield (head + skip_tail, k-i)\n",
    "            \n",
    "def hal(corpus, window):\n",
    "    skips = skipgrams(corpus, 2, window)\n",
    "    #print(skips)\n",
    "    X = pd.DataFrame(skips, columns=['skipgram', 'weight'])\n",
    "    X[['word1', 'word2']] = X['skipgram'].apply(pd.Series)\n",
    "    #print(X)\n",
    "    X.drop('skipgram', axis=1, inplace=True)\n",
    "    X = X.groupby(['word1', 'word2']).sum().unstack().fillna(0).astype(int)\n",
    "    X.columns = X.columns.levels[1].values\n",
    "    X.index = X.index.values\n",
    "    return X.T\n",
    "\n",
    "mycorpus = 'The horse raced past the barn fell .'.split()\n",
    "mycorpus = [word.lower() for word in mycorpus]\n",
    "\n",
    "hal(mycorpus, 5).to_dict()\n",
    "#list(ngrams(mycorpus, 2+5, pad_right=True, right_pad_symbol=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "LOG_BASE = 2\n",
    "\n",
    "def run_hal(train_data):\n",
    "    \n",
    "    for i in range(len(train_data)):\n",
    "        \n",
    "        sentence_size = len(train_data[i])\n",
    "        \n",
    "        for j in range(sentence_size):\n",
    "            \n",
    "            key = train_data[i][j]\n",
    "            \n",
    "            # compute start of sliding window\n",
    "            start_ind = 0\n",
    "            if j - WINDOW_SIZE >= 0:\n",
    "                start_ind = j - WINDOW_SIZE\n",
    "                \n",
    "            # compute end of sliding window\n",
    "            end_ind = sentence_size\n",
    "            if j + WINDOW_SIZE + 1 <= sentence_size:\n",
    "                end_ind = j + WINDOW_SIZE + 1\n",
    "                \n",
    "            for index in range(start_ind, end_ind, 1):\n",
    "                \n",
    "                if j == index:\n",
    "                    continue\n",
    "                    \n",
    "                weight = 1 / abs(j - index)\n",
    "                idf = math.log((num_words / occ[train_data[i][index]]), LOG_BASE)\n",
    "                if train_data[i][index] in voc[key]:\n",
    "                    voc[key][train_data[i][index]] += weight * idf\n",
    "                else:\n",
    "                    voc[key][train_data[i][index]] = 0\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import re\n",
    "import unidecode\n",
    "import math\n",
    "from scipy.sparse import lil_matrix\n",
    "import operator\n",
    "# sorting map\n",
    "import numpy as np\n",
    "# local imports\n",
    "import czech_stemmer as stem\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "############################ CONSTANTS AND LOADING DATA\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train.txt'\n",
    "STOPWORDS_PATH = '../data/stopwords.txt'\n",
    "REGEX = re.compile('[^a-zA-Z]')\n",
    "WINDOW_SIZE = 2\n",
    "LOG_BASE = 2\n",
    "TOP_WORD_OCCUR = 3000\n",
    "TOP_RESULTS = 10\n",
    "\n",
    "# load data from txt files except stopwords (without accents)\n",
    "with open(STOPWORDS_PATH, encoding=\"utf8\") as file:\n",
    "    stopwords = file.readlines()\n",
    "with open(TRAIN_DATA_PATH, encoding=\"utf8\") as file:\n",
    "    train_data = file.readlines()\n",
    "\n",
    "############################ METHODS\n",
    "\n",
    "# preprocess string    \n",
    "def to_string(str):\n",
    "    # remove accents and lower case\n",
    "    without_accents = unidecode.unidecode(str).lower()\n",
    "    # keep only a-z\n",
    "    without_accents = REGEX.sub('', without_accents)\n",
    "    return without_accents\n",
    "\n",
    "# check if is string among stopwords if yes returns empty string\n",
    "# otherwise returns string\n",
    "def remove_stopwords(str):\n",
    "    # if is str in our stopwords return '' otherwise return str\n",
    "    if any(str in s for s in stopwords):\n",
    "        return ''\n",
    "    return str\n",
    "\n",
    "# expected list of sentences which tokenize\n",
    "# preprocess and returns\n",
    "def preprocess_sentences(loaded_sentences, use_stopwords = True, \n",
    "                         use_stemm = False):\n",
    "    sentences = []\n",
    "    for i in range(len(loaded_sentences)):\n",
    "        splitted = loaded_sentences[i].split()\n",
    "        preprocessed_sentence = []\n",
    "        for j in range(len(splitted)):\n",
    "            formatted = to_string(splitted[j])\n",
    "            without_stopwords = formatted\n",
    "            if use_stopwords:\n",
    "                without_stopwords = remove_stopwords(formatted)\n",
    "            if (without_stopwords == ''):\n",
    "                continue\n",
    "            # stemming can be agressive or not according to flag\n",
    "            stemmed = without_stopwords\n",
    "            if use_stemm:\n",
    "                stemmed = stem.cz_stem(stemmed)\n",
    "            preprocessed_sentence.append(stemmed)\n",
    "        sentences.append(preprocessed_sentence)\n",
    "    return sentences\n",
    "\n",
    "# build scipy sparse matrix which represents ccoocurencest matrix\n",
    "def build_sparse_matrix(voc, indexes):\n",
    "    voc_size = len(voc)\n",
    "    mat = lil_matrix((voc_size, voc_size), dtype=float)\n",
    "    for key, value in voc.items():\n",
    "        for key2, value2 in value.items(): \n",
    "            mat[indexes[key], indexes[key2]] = value2\n",
    "    return mat\n",
    "\n",
    "# compute cosine similarity between pivot matrix and all others words matrix   \n",
    "def compute_results(indexes, context_word, sparse_matrix):\n",
    "    # idnex of pivot word in sparse matrix\n",
    "    ind = indexes[context_word]\n",
    "    # convert sparse matrix row to long array for pivot\n",
    "    mat = sparse_matrix[:, ind]\n",
    "    results = {}\n",
    "    # iterate through vocabulary\n",
    "    for key, ind2 in indexes.items():\n",
    "        # convert sparse matrix row to long array for current item\n",
    "        mat2 = sparse_matrix[:, ind2]\n",
    "        results[key] = cosine_similarity(np.transpose(mat), np.transpose(mat2), dense_output=False)\n",
    "    return results\n",
    "\n",
    "# select only TOP most frequented words\n",
    "def select_top_occurs(train_data, occ):\n",
    "    for i in range(len(train_data)):\n",
    "        for j in range(len(train_data[i])):\n",
    "            if train_data[i][j] in occ:\n",
    "                occ[train_data[i][j]] += 1\n",
    "            else:\n",
    "                occ[train_data[i][j]] = 1\n",
    "                \n",
    "    sorted_occ = sorted(occ.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_occ = sorted_occ[0:TOP_WORD_OCCUR]\n",
    "    sorted_occ = dict(sorted_occ)\n",
    "    smaller_data = []\n",
    "    for i in range(len(train_data)):\n",
    "        reduced_sentence = []\n",
    "        for j in range(len(train_data[i])):\n",
    "            if train_data[i][j] in sorted_occ:\n",
    "                reduced_sentence.append(train_data[i][j])\n",
    "        if len(reduced_sentence) > 0:\n",
    "            smaller_data.append(reduced_sentence)\n",
    "    occ = sorted_occ\n",
    "    train_data = smaller_data\n",
    "    return train_data\n",
    "\n",
    "\n",
    "# print results    \n",
    "def print_results(res):\n",
    "    # convert data from sparse matrix into single var\n",
    "    for key, value in res.items(): \n",
    "        res[key] = value.toarray().flatten()[0]\n",
    "    top_results = sorted(res.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    top_results = top_results[0:TOP_RESULTS]\n",
    "    print(\"##### TOP \" + str(TOP_RESULTS) + \" results for word '\" + context_word + \"'\")\n",
    "    for i in range(len(top_results)):\n",
    "        print(top_results[i][0] + \": %0.3f\" % top_results[i][1])    \n",
    "\n",
    "# find counts of neighbours for building coocurency matrix        \n",
    "def run_hal(train_data):\n",
    "    for i in range(len(train_data)):\n",
    "        sentence_size = len(train_data[i])\n",
    "        for j in range(sentence_size):\n",
    "            key = train_data[i][j]\n",
    "            # compute start of sliding window\n",
    "            start_ind = 0\n",
    "            if j - WINDOW_SIZE >= 0:\n",
    "                start_ind = j - WINDOW_SIZE\n",
    "            # compute end of sliding window\n",
    "            end_ind = sentence_size\n",
    "            if j + WINDOW_SIZE + 1 <= sentence_size:\n",
    "                end_ind = j + WINDOW_SIZE + 1\n",
    "            for index in range(start_ind, end_ind, 1):\n",
    "                if j != index:\n",
    "                    weight = 1 / abs(j - index)\n",
    "                    idf = math.log((num_words / occ[train_data[i][index]]), LOG_BASE)\n",
    "                    if train_data[i][index] in voc[key]:\n",
    "                        voc[key][train_data[i][index]] += weight * idf\n",
    "                    else:\n",
    "                        voc[key][train_data[i][index]] = 0\n",
    "                        \n",
    "############################ MAIN   \n",
    "                        \n",
    "# remove accents, lower case and eventually stem or remove stopwords\n",
    "train_data = preprocess_sentences(train_data)\n",
    "# create smaller dataset for faster run\n",
    "occ = {}\n",
    "train_data = select_top_occurs(train_data, occ)\n",
    "# create vocabulary and map of occurences for computing IDF\n",
    "voc = {}\n",
    "indexes = {}\n",
    "counter = 0\n",
    "num_words = 0\n",
    "for i in range(len(train_data)):\n",
    "    for j in range(len(train_data[i])):\n",
    "        num_words += 1\n",
    "        voc[train_data[i][j]] = {}\n",
    "        if train_data[i][j] not in indexes:\n",
    "            indexes[train_data[i][j]] = counter\n",
    "            counter += 1\n",
    "# process HAL\n",
    "run_hal(train_data)\n",
    "# building sparse matrix - we save memory space\n",
    "sparse_matrix = build_sparse_matrix(voc, indexes)\n",
    "# compute cosine similarity for pivot\n",
    "context_word = 'washington'\n",
    "res = compute_results(indexes, context_word, sparse_matrix)\n",
    "# print results\n",
    "print_results(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/geoffbacon/semrep/blob/366d5740a117f47cda73807a8b9e6b7cf1ca8138/semrep/models/hal/HAL.ipynb\n",
    "\n",
    "https://github.com/fozziethebeat/S-Space/blob/master/src/main/java/edu/ucla/sspace/hal/HyperspaceAnalogueToLanguage.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  public void  processDocument(BufferedReader document) throws IOException {\n",
    "        Queue<String> nextWords = new ArrayDeque<String>();\n",
    "        Queue<String> prevWords = new ArrayDeque<String>();\n",
    "\n",
    "        Iterator<String> documentTokens =\n",
    "            IteratorFactory.tokenizeOrdered(document);\n",
    "\n",
    "        String focus = null;\n",
    "\n",
    "        // Rather than updating the matrix every time an occurrence is seen,\n",
    "        // keep a thread-local count of what needs to be modified in the matrix\n",
    "        // and update after the document has been processed.  This saves\n",
    "        // potential contention from concurrent writes.\n",
    "        Map<Pair<Integer>,Double> matrixEntryToCount =\n",
    "            new HashMap<Pair<Integer>,Double>();\n",
    "\n",
    "        //Load the first windowSize words into the Queue\n",
    "        for(int i = 0;  i < windowSize && documentTokens.hasNext(); i++)\n",
    "            nextWords.offer(documentTokens.next());\n",
    "\n",
    "        while(!nextWords.isEmpty()) {\n",
    "\n",
    "            // Load the top of the nextWords Queue into the focus word\n",
    "            focus = nextWords.remove();\n",
    "\n",
    "            // Add the next word to nextWords queue (if possible)\n",
    "            if (documentTokens.hasNext())\n",
    "                nextWords.offer(documentTokens.next());\n",
    "\n",
    "            // If the filter does not accept this word, skip the semantic\n",
    "            // processing, continue with the next word\n",
    "            if (!focus.equals(IteratorFactory.EMPTY_TOKEN)) {\n",
    "                int focusIndex = termToIndex.getDimension(focus);\n",
    "                // Only process co-occurrences with words with non-negative\n",
    "                // dimensions.\n",
    "                if (focusIndex >= 0) {\n",
    "                    // in front of the focus word\n",
    "                    int wordDistance = -windowSize + (windowSize - prevWords.size());\n",
    "                    addTokens(prevWords, focusIndex, wordDistance, matrixEntryToCount);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            // last, put this focus word in the prev words and shift off the\n",
    "            // front if it is larger than the window\n",
    "            prevWords.offer(focus);\n",
    "            if (prevWords.size() > windowSize)\n",
    "                prevWords.remove();\n",
    "        }\n",
    "\n",
    "        // Once the document has been processed, update the co-occurrence matrix\n",
    "        // accordingly.\n",
    "        for (Map.Entry<Pair<Integer>,Double> e : matrixEntryToCount.entrySet()){\n",
    "            Pair<Integer> p = e.getKey();\n",
    "            cooccurrenceMatrix.addAndGet(p.x, p.y, e.getValue());\n",
    "        }\n",
    "    }\n",
    "\n",
    "    '''\n",
    "     * Adds co-occurrence counts between the list of previous words in {@code\n",
    "     * words} and the focus word represented by {@code focusIndex} which start\n",
    "     * at {@code distance} tokens away from the focus word.  All Counts will be\n",
    "     * added into {@code matrixEntryToCount}.\n",
    "    '''\n",
    "\n",
    "def addTokens(words, focusIndex, distance, matrix, i, j):\n",
    "    for word in words:\n",
    "        # skip adding co-occurence values for words that are not  accepted by the filter\n",
    "        # Get the current number of times that the focus word has\n",
    "        # co-occurred with this word before after it.  Weight the\n",
    "        # word appropriately baed on distance\n",
    "        value = weighting.weight(distance, windowSize);\n",
    "        matrix[index, focusIndex] += weighting.weight(distance, windowSize)\n",
    "        distance++;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def sliding_window(seq, n):\n",
    "    \"Returns a sliding centered window of size +/-n from (of width 2 * n + 1) \"\n",
    "    y = [None] * n + list(seq) + [None] * n\n",
    "    for i in range(n, len(y)+n):\n",
    "        yield y[i-n:i+n+1]\n",
    "\n",
    "def sliding_window_it(it, n):\n",
    "    it = itertools.chain([None] * n, it, [None] * n * 2)\n",
    "    tail = tuple(itertools.islice(it, n))\n",
    "    head = tuple(itertools.islice(it, n+1))\n",
    "    for v in it:\n",
    "        yield list(tail + head + (v,))\n",
    "        tail = tail[1:] + (head[0],)\n",
    "        head = head[1:] + (v,)\n",
    "\n",
    "def sliding_window3(seq, n=2):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(itertools.islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.help(hv.GraphNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

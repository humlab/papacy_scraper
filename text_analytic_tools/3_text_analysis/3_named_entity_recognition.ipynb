{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culture of International Relations - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os #, collections, zipfile\n",
    "#import re, typing.re\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import text_corpus\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import domain_logic_vatican as domain_logic\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "DATA_FOLDER, PATTERN = '../../data',  '*.txt'\n",
    "DF_TAGSET = pd.read_csv(os.path.join(DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set_matplotlib_formats('svg')   \n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    # FIXME VARYING ASPECTS: document_index = WTI_INDEX for tCoIR\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, document_index=None, container=container, compute_ner=True, domain_logic=domain_logic)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display Named Entities\n",
    "import gui_utility\n",
    "from spacy import displacy\n",
    "\n",
    "def display_document_entities_gui(corpus, document_index):\n",
    "    \n",
    "    # FIXME VARYING ASPECT: Add \"document_name\" to document_index, or function that creates name\n",
    "    filenames = document_index.filename\n",
    "    document_options = list(sorted(zip(filenames,filenames.index), key=lambda x: x[0]))\n",
    "\n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        document_id=widgets.Dropdown(description='Document', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='50%')),\n",
    "        left=widgets.Button(description='<<', button_style='Success', layout=widgets.Layout(width='40px')),\n",
    "        right=widgets.Button(description='>>', button_style='Success', layout=widgets.Layout(width='40px')),\n",
    "    )\n",
    "\n",
    "    def display_document_entities(corpus, document_id):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:        \n",
    "            doc = textacy_utility.get_document_by_id(corpus, document_id)\n",
    "            displacy.render(doc, style='ent', jupyter=True)\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.document_id, gui.left, gui.right]),\n",
    "        widgets.VBox([gui.output], layout=widgets.Layout(margin_top='20px', height='600px',width='100%'))\n",
    "    ]))\n",
    "    \n",
    "    itw = widgets.interactive(\n",
    "        display_document_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        document_id=gui.document_id\n",
    "    )\n",
    "    \n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    corpus = current_corpus()\n",
    "    document_index = domain_logic.compile_documents(corpus)\n",
    "    display_document_entities_gui(corpus, document_index=document_index)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(doc, include_types=None, drop_determiners=True):\n",
    "    \n",
    "    entities = (x for x in doc.ents if not x.text.isspace())\n",
    "    \n",
    "    if include_types is not None:\n",
    "        assert isinstance(include_types, (set, list, tuple))\n",
    "        entities = (x for x in entities if x.label_ in include_types)\n",
    "\n",
    "    if drop_determiners is True:\n",
    "        entities = (x if x[0].pos != DET else SpacySpan(x.doc, x.start + 1, x.end, label=x.label, vector=x.vector) for x in entities)\n",
    "\n",
    "    for x in entities:\n",
    "        yield x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Extract Named Entities<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file run_ner_places.py\n",
    "#from cytoolz import itertoolz\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import domain_logic_vatican as domain_logic\n",
    "import text_corpus\n",
    "import logging\n",
    "import string\n",
    "\n",
    "from spacy.tokens.span import Span as SpacySpan\n",
    "\n",
    "logger = logging.getLogger('ner')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "NON_PRINT_CHARS = set([chr(i) for i in range(128)]).difference(string.printable)\n",
    "\n",
    "DEL_WS_CHARS = str.maketrans('', '', '\"\\n\\t')\n",
    "DEL_CRAP_CHARS = str.maketrans('', '', '\"\\n\\t?@')# + NON_PRINT_CHARS)\n",
    "#DEL_NON_PRINT = str.maketrans('', '', NON_PRINT_CHARS)\n",
    "\n",
    "#def get_doc_places(doc):\n",
    "#    return ( (w.text, w.lemma_, str(w.lemma_).translate(DEL_CRAP_CHARS), w[0].ent_type_)\n",
    "#                for w in doc.ents if w[0].ent_type_ in ['LOC', 'GPE'] and w.lemma_.strip() != '' )\n",
    "\n",
    "def get_doc_place_entities(doc, drop_determiner=True):\n",
    "    \n",
    "    DET = spacy.parts_of_speech.DET\n",
    "    PUNCT = spacy.parts_of_speech.PUNCT\n",
    "    \n",
    "    include_types = ['LOC', 'GPE']\n",
    "    \n",
    "    entities = doc.ents\n",
    "    entities = (x for x in entities if not x.text.isspace())\n",
    "    entities = (x for x in entities if x.lemma_.strip() != '')\n",
    "    entities = (x for x in entities if x.label_ in include_types)\n",
    "\n",
    "    if drop_determiner is True:\n",
    "        entities = (x if x[0].pos != DET else SpacySpan(x.doc, x.start + 1, x.end, label=x.label, vector=x.vector) for x in entities)\n",
    "        \n",
    "    for x in entities:\n",
    "        text = x.text.translate(DEL_WS_CHARS)\n",
    "        lemma = x.lemma_.translate(DEL_WS_CHARS)\n",
    "        #tidy_lemma = lemma.translate(DEL_CRAP_CHARS).replace('  ', ' ')\n",
    "        tidy_lemma = lemma.replace('  ', ' ')\n",
    "        yield (text, lemma, tidy_lemma, x[0].ent_type_)\n",
    "        \n",
    "#def get_corpus_places(corpus):\n",
    "#    return itertoolz.chain.from_iterable(( get_doc_places(doc) for doc in corpus ))\n",
    "\n",
    "def create_source_stream(source_path, lang, document_index=None):\n",
    "    reader = text_corpus.CompressedFileReader(source_path)\n",
    "    stream = domain_logic.get_document_stream(reader, lang, document_index=document_index)\n",
    "    return stream\n",
    "\n",
    "def create_nlp(model='en_core_web_sm', disable=None):\n",
    "    nlp = spacy.load(model, disable=disable)\n",
    "    nlp.tokenizer = textacy_utility.keep_hyphen_tokenizer(nlp)\n",
    "    return nlp\n",
    "\n",
    "source_paths = [ '../../data/benedict-xvi_curated_20190326.txt_preprocessed.zip', '../../data/francis_curated_20190326.txt_preprocessed.zip' ]\n",
    "\n",
    "model_name = 'en_core_web_lg'\n",
    "\n",
    "logger.info('Loading model {}'.format(model_name))\n",
    "nlp = create_nlp(model=model_name, disable=('parser', 'textcat'))\n",
    "\n",
    "for source_path in source_paths:\n",
    "    logger.info('Processing {}...'.format(source_path))\n",
    "    stream = create_source_stream(source_path, 'en')\n",
    "    file_counter = 0\n",
    "    places = []\n",
    "    for filename, text, _ in stream:\n",
    "        file_counter += 1\n",
    "        doc = nlp(text)\n",
    "        places.extend(list(get_doc_place_entities(doc)))\n",
    "        if file_counter % 100 == 0:\n",
    "            logger.info('Processed {} files...{} places found...'.format(file_counter, len(places)))\n",
    "            #break\n",
    "        doc = None\n",
    "        \n",
    "df = pd.DataFrame(places, columns=['text', 'lemma', 'tidy_lemma', 'ent_type'])\n",
    "df.to_csv('./NER_with_tagging_total.txt', sep='\\t')\n",
    "\n",
    "df_grouped = df.groupby(['tidy_lemma', 'ent_type']).size().reset_index()\n",
    "\n",
    "df_grouped.to_csv('./NER_with_tagging_total_tidy_lemma_grouped.txt', sep='\\t')\n",
    "\n",
    "\n",
    "#[z for z in textacy.extract.entities(doc)]\n",
    "#[[ ent for ent in textacy.extract.entities(doc) ] for doc in corpus if len(doc.ents or []) > 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nohup python3 run_ner_places.py &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> Display Named Entity Statistics<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compile_named_entity_data(corpus, document_index, drop_determiners=True, min_freq=1):\n",
    "    #textacy.extract.entities(doc, include_types=None, exclude_types=None, drop_determiners=True, min_freq=1)\n",
    "    data = [[\n",
    "        (doc._.meta['document_id'], ent[0].ent_type_, ent.text, ent.lemma_)\n",
    "             for ent in textacy.extract.entities(doc, exclude_types=('CARDINAL',), drop_determiners=drop_determiners, min_freq=min_freq) ]\n",
    "                for doc in corpus\n",
    "    ]\n",
    "    data = utility.flatten(data)\n",
    "    df = pd.DataFrame(data, columns=['document_id', 'ent_type', 'text', 'lemma']).set_index('document_id')\n",
    "    df = pd.merge(df, document_index, left_index=True, right_index=True, how='inner')\n",
    "    return df[df.year > 0][['pope', 'year', 'genre', 'ent_type', 'text', 'lemma', 'filename']].reset_index()\n",
    "\n",
    "def display_grouped_by_entities_gui(corpus, document_index):\n",
    "    \n",
    "    columns = compile_named_entity_data([corpus[0]], document_index).columns\n",
    "    \n",
    "    group_by_options = [ (x.title(), x) for x in columns if x not in [ 'ent_type', 'text', 'lemma', 'filename', 'index'] ]\n",
    "    group_by_values = [ x for _, x in group_by_options ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        group_by=widgets.SelectMultiple(description='Group by', options=group_by_options, value=group_by_values, rows=3, layout=widgets.Layout(width='180px')),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=True, description='Drop DET',  tooltip='Drop_determiners`', icon='check'),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        min_freq=widgets.IntSlider(description='Min freq', min=1, max=10, value=2, step=1, layout=widgets.Layout(width='400px')),\n",
    "    )\n",
    "        \n",
    "    def display_grouped_by_entities(corpus, group_by, drop_determiners, min_freq):\n",
    "        gui.output.clear_output()\n",
    "        named_entities = compile_named_entity_data(corpus, document_index, drop_determiners, min_freq)\n",
    "        with gui.output:\n",
    "            df = named_entities.groupby(list(group_by) + ['ent_type', 'lemma']).size().reset_index()\n",
    "            df = df.rename(columns={0:'Count'})\n",
    "            df = df.sort_values('Count', ascending=False)\n",
    "            display(df)\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_grouped_by_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        #named_entities=widgets.fixed(named_entities),\n",
    "        group_by=gui.group_by,\n",
    "        drop_determiners=gui.drop_determiners,\n",
    "        min_freq=gui.min_freq\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.group_by, gui.drop_determiners, gui.min_freq]),\n",
    "        widgets.VBox([gui.output]),\n",
    "        itw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    \n",
    "try:\n",
    "    corpus = current_corpus()\n",
    "    document_index = domain_logic.compile_documents(corpus)\n",
    "    display_grouped_by_entities_gui(corpus, document_index)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Stanford NER Tagger (CoreNLP)<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>PREPARE</span> Verify that Stanford CoreNLP is up and running<span style='color: green; float: right'>SKIP</span>\n",
    "Stanford CoreNLP server must be started as described in:  https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n",
    "\n",
    "With docker:\n",
    "```bash\n",
    "docker pull frnkenstien/corenlp\n",
    "docker run -p 9000:9000 --name coreNLP --rm -i -t frnkenstien/corenlp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANFORD_CORE_NLP_URL = 'http://localhost:9000'\n",
    "\n",
    "try:\n",
    "    from nltk.parse import corenlp\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url=STANFORD_CORE_NLP_URL, encoding='utf8', tagtype='ner')\n",
    "    input_tokens = 'Stony Brook University in NY'.split()\n",
    "    tagged_output = corenlp_tagger.tag(input_tokens)\n",
    "    print('Stanford tagger is up and running!')\n",
    "    print(' Result: ' + ' '.join([ x + '/' + y for x,y in tagged_output]))\n",
    "except: # (ConnectionError, ConnectionRefusedError):\n",
    "    logger.error('Server not found! Please start Stanford CoreNLP Server!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file run_stanford_ner.py\n",
    "import os, sys, time\n",
    "import glob\n",
    "import types\n",
    "import ipywidgets as widgets\n",
    "import text_corpus\n",
    "import domain_logic_vatican as domain_logic\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import common.widgets_config as widgets_config\n",
    "import common.utility as utility\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "from nltk.parse import corenlp\n",
    "\n",
    "STANFORD_CORE_NLP_URL = 'http://localhost:9000'\n",
    "\n",
    "def merge_entities(entities):\n",
    "    n_entities = len(entities)\n",
    "    if n_entities <= 1:\n",
    "        return entities\n",
    "    merged = entities[:1]\n",
    "    for doc_id, i_n, w_n, t_n in entities[1:]:\n",
    "        doc_id_p, i_p, w_p, t_p = merged[-1]\n",
    "        if i_n == i_p + 1 and t_n == t_p:\n",
    "            merged[-1] = (doc_id, i_n, '_'.join([w_p, w_n]), t_p)\n",
    "        else:\n",
    "            merged.append((doc_id, i_n, w_n, t_n))\n",
    "    return merged\n",
    "\n",
    "def recognize_named_entities(tagger, doc_id, text, excludes=None, includes=None):\n",
    "    ''' excludes not used '''\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    start_index = 0\n",
    "    excludes = excludes or []\n",
    "    includes = includes or []\n",
    "    merged_ents = []\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged_data = tagger.tag(tokens)\n",
    "        ents = [ (doc_id, start_index + index, word, ent_type) for index, (word, ent_type) \\\n",
    "                in enumerate(tagged_data) if ent_type in includes ]\n",
    "        start_index += len(sentence)\n",
    "        merged_ents.extend(merge_entities(ents))\n",
    "    return merged_ents\n",
    "           \n",
    "def compute_stanford_ner(source_file, service_url=STANFORD_CORE_NLP_URL, excludes=None, includes=None):\n",
    "    \n",
    "    includes = includes or (\n",
    "        'LOCATION', 'CITY', 'STATE_OR_PROVINCE', 'COUNTRY'\n",
    "    )\n",
    "    \n",
    "    # For English, by default, this annotator recognizes named (PERSON, LOCATION, ORGANIZATION, MISC),\n",
    "    # numerical (MONEY, NUMBER, ORDINAL, PERCENT), and temporal (DATE, TIME, DURATION, SET) entities (12 classes).\n",
    "    # Adding the regexner annotator and using the supplied RegexNER pattern files adds support for the fine-grained and additional entity \n",
    "    # classes EMAIL, URL, CITY, STATE_OR_PROVINCE, COUNTRY, NATIONALITY, RELIGION, (job) \n",
    "    # TITLE, IDEOLOGY, CRIMINAL_CHARGE, CAUSE_OF_DEATH (11 classes) for a total of 23 classes.\n",
    "\n",
    "    assert os.path.isfile(source_file), 'File missing!'\n",
    "    \n",
    "    tagger = corenlp.CoreNLPParser(url=service_url, encoding='utf8', tagtype='ner')\n",
    "    #tokenizer = corenlp.CoreNLPParser(url=service_url, encoding='utf8')\n",
    "    \n",
    "    reader = text_corpus.CompressedFileReader(source_file)\n",
    "    document_index = domain_logic.compile_documents_by_filename(reader.filenames)\n",
    "    stream = domain_logic.get_document_stream(reader, 'en', document_index=document_index)\n",
    "    \n",
    "    i = 0\n",
    "    ner_data = []\n",
    "    for filename, text, metadata in stream:\n",
    "        print(filename)\n",
    "        document_id = document_index.loc[document_index.filename == filename, 'document_id'].values[0]\n",
    "        ner = recognize_named_entities(tagger, document_id, text, excludes, includes)\n",
    "        ner_data.extend(ner)\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            logger.info('Processed {} files...'.format(i))\n",
    "            # break\n",
    "        \n",
    "    return ner_data\n",
    "\n",
    "def compute_and_store_stanford_ner(source_file):\n",
    "    \n",
    "    ner_data = compute_stanford_ner(source_file=source_file)\n",
    "\n",
    "    df = pd.DataFrame(ner_data, columns=['doc_id', 'pos', 'entity', 'ent_type'])\n",
    "    df.index.name = 'id'\n",
    "\n",
    "    store_name = 'ner_{}_{}.txt'.format(\n",
    "        os.path.splitext(os.path.split(source_file)[1])[0],\n",
    "        time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "    )\n",
    "\n",
    "    df.to_csv(store_name, sep='\\t')\n",
    "    logger.info('Result stored in %s', store_name)\n",
    "    return df\n",
    "        \n",
    "def display_stanford_ner_gui(data_folder):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    corpus_files = sorted(glob.glob(os.path.join(data_folder, '*.txt.zip')))\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        source_path=widgets_config.dropdown(description='Corpus', options=corpus_files, value=corpus_files[-1], layout=lw('300px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('100px'))\n",
    "    )\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.source_path,\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.compute,\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ]))\n",
    "    \n",
    "    def compute_stanford_ner_callback(*_args):\n",
    "        \n",
    "        gui.output.clear_output()\n",
    "        \n",
    "        with gui.output:\n",
    "            df = compute_and_store_stanford_ner(gui.source_path.value)\n",
    "            display(df)\n",
    "            \n",
    "    gui.compute.on_click(compute_stanford_ner_callback)\n",
    "\n",
    "\n",
    "#data_folder = '../../data'\n",
    "#display_stanford_ner_gui(data_folder)\n",
    "for source_file in [ '../../data/francis_curated_20190326.txt.zip', '../../data/benedict-xvi_curated_20190326.txt.zip', ]:\n",
    "    compute_and_store_stanford_ner(source_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_index(path):\n",
    "    reader = text_corpus.CompressedFileReader(path)\n",
    "    df = domain_logic.compile_documents_by_filename(reader.filenames)\n",
    "    df['document_id'] = df.document_id.astype(np.int32)\n",
    "    df = df.rename(columns={'document_id': 'doc_id'})\n",
    "    df = df.set_index('doc_id')\n",
    "    df = df[['pope', 'genre', 'year', 'filename']]\n",
    "    return df\n",
    "\n",
    "def load_ner_result(path):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    df['doc_id'] = df.doc_id.astype(np.int32)\n",
    "    df = df.set_index('id')\n",
    "    return df\n",
    "\n",
    "def compile_ner_result(source_path, result_path):\n",
    "    document_index = load_document_index(source_path)\n",
    "    df_ner = load_ner_result(result_path)\n",
    "    df_agg = df_ner.groupby(['doc_id', 'ent_type', 'entity']).size().reset_index().rename(columns={0: 'entity_count'})\n",
    "    df_doc_agg = document_index.merge(df_agg, left_index=True, right_on='doc_id', how='inner')\n",
    "    return df_doc_agg\n",
    "\n",
    "df_francis = compile_ner_result('../../data/benedict-xvi_curated_20190326.txt.zip', 'ner_benedict-xvi_curated_20190326.txt_20190319193045.txt')\n",
    "df_benedict = compile_ner_result('../../data/francis_curated_20190326.txt.zip', 'ner_francis_curated_20190326.txt_20190326145420.txt')\n",
    "\n",
    "df_popes = pd.concat([df_francis, df_benedict], ignore_index=True, axis=0)\n",
    "\n",
    "df_popes_entity_agg = df_popes.groupby(['pope', 'entity']).agg({'entity_count': 'sum'}).reset_index()\n",
    "df_entity_agg = df_popes.groupby(['entity']).agg({'entity_count': 'sum'}).reset_index()\n",
    "\n",
    "print(df_popes_entity_agg.entity_count.sum(), df_entity_agg.entity_count.sum())\n",
    "    \n",
    "with pd.ExcelWriter('ner_francis_and_benedict_stanford_20190326.xlsx') as writer:\n",
    "    df_popes.to_excel(writer, sheet_name='Pivot (document)'),\n",
    "    df_popes_entity_agg.to_excel(writer, sheet_name='Pivot (pope, entity)'),\n",
    "    df_entity_agg.to_excel(writer, sheet_name='Pivot (entity)'),\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ner[df_ner.doc_id == 3]\n",
    "df_agg = df_ner.groupby(['doc_id', 'ent_type', 'entity']).size().reset_index().rename(columns={0: 'count'})\n",
    "df_agg\n",
    "\n",
    "df_pope_year_genre_type_agg = df_doc_agg.groupby(['pope', 'year', 'genre', 'ent_type', 'entity']).agg({'count': 'sum'}).reset_index()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import text_corpus\n",
    "import textacy.keyterms\n",
    "import gui_utility\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n",
    "current_document_index = lambda: current_corpus_container().document_index\n",
    "\n",
    "from domain_logic_config import current_domain as domain_logic\n",
    "\n",
    "DF_TAGSET = pd.read_csv(os.path.join(domain_logic.DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    textacy_corpus_gui.display_corpus_load_gui(domain_logic.DATA_FOLDER, container=container, domain_logic=domain_logic)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Find Key Terms <span style='float: right; color: green'>OPTIONAL</span>\n",
    "- [TextRank]\tMihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- [SingleRank]\tHasan, K. S., & Ng, V. (2010, August). Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.\n",
    "- [RAKE]\tRose, S., Engel, D., Cramer, N., & Cowley, W. (2010). Automatic Keyword Extraction from Individual Documents. In M. W. Berry & J. Kogan (Eds.), Text Mining: Theory and Applications: John Wiley & Son\n",
    "https://github.com/csurfer/rake-nltk\n",
    "https://github.com/aneesha/RAKE\n",
    "https://github.com/vgrabovets/multi_rake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color: green'>PREPARE/DESCRIBE </span>RAKE <span style='float: right; color: green'>WORK IN PROGRESS</span>\n",
    "\n",
    "https://github.com/JRC1995/RAKE-Keyword-Extraction\n",
    "https://github.com/JRC1995/TextRank-Keyword-Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Document Key Terms\n",
    "from rake_nltk import Rake, Metric\n",
    "import string\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import gui_utility\n",
    "\n",
    "def textacy_rake(doc, language='english', normalize='lemma', n_keyterms=20, stopwords=None, metric=Metric.DEGREE_TO_FREQUENCY_RATIO):\n",
    "    \n",
    "    punctuations = string.punctuation + \"\\\"\"\n",
    "    r = Rake(\n",
    "        stopwords=stopwords,  # NLTK stopwords if None\n",
    "        punctuations=punctuations, # NLTK by default\n",
    "        language=language,\n",
    "        ranking_metric=metric,\n",
    "        max_length=100000,\n",
    "        min_length=1\n",
    "    )\n",
    "    text = ' '.join([ x.lemma_ for x in doc ] if normalize == 'lemma' else [ x.lower_ for x in doc ])\n",
    "    r.extract_keywords_from_text(doc.text)\n",
    "    keyterms = [ (y, x) for (x, y) in r.get_ranked_phrases_with_scores() ]\n",
    "    return keyterms[:n_keyterms]\n",
    "\n",
    "def display_rake_gui(corpus, language):\n",
    "    \n",
    "    lw = lambda width: widgets.Layout(width=width)\n",
    "\n",
    "    document_index =  domain_logic.compile_documents(corpus)\n",
    "\n",
    "    filenames = document_index.filename\n",
    "    document_options = [('all documents', -1)] + list(sorted(zip(filenames, filenames.index), key=lambda x: x[0]))\n",
    "\n",
    "    metric_options = [\n",
    "        ('Degree / Frequency', Metric.DEGREE_TO_FREQUENCY_RATIO),\n",
    "        ('Degree', Metric.WORD_DEGREE),\n",
    "        ('Frequency', Metric.WORD_FREQUENCY)\n",
    "    ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        progress=widgets.IntProgress(min=0, max=1, step=1, layout=lw(width='95%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=10, step=1, layout=lw(width='340px')),\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=lw(width='40%')),\n",
    "        metric=widgets.Dropdown(description='Metric', options=metric_options, value=Metric.DEGREE_TO_FREQUENCY_RATIO, layout=lw(width='300px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=lw(width='160px')),\n",
    "        left=widgets.Button(description='<<', button_style='Success', layout=lw('40px')),\n",
    "        right=widgets.Button(description='>>', button_style='Success', layout=lw('40px')),\n",
    "    )\n",
    "    \n",
    "    def compute_textacy_rake(corpus, document_id, language, normalize, n_keyterms, metric):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            docs = corpus if document_id == -1 else [ textacy_utility.get_document_by_id(corpus, document_id) ]\n",
    "            gui.progress.max = len(docs)\n",
    "            gui.progress.value = 0\n",
    "            phrase_data = []\n",
    "            \n",
    "            for doc in docs:\n",
    "                filename = doc._.meta['filename']\n",
    "                phrases = textacy_rake(doc, language=language, normalize=normalize, n_keyterms=n_keyterms, stopwords=None, metric=metric)\n",
    "                phrase_data.extend(\n",
    "                    [ (filename, i, x[0], x[1]) for i,x in enumerate(phrases)]\n",
    "                )\n",
    "                gui.progress.value += 1\n",
    "                \n",
    "            df = pd.DataFrame(phrase_data, columns=['filename', 'rank', 'phrase', 'score'])\n",
    "            display(df)\n",
    "            gui.progress.value = 0\n",
    "        return df\n",
    "    \n",
    "    itw = widgets.interactive(\n",
    "        compute_textacy_rake,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        document_id=gui.document_id,\n",
    "        language=widgets.fixed(language),\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "        metric=gui.metric\n",
    "    )\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([gui.document_id, gui.left, gui.right, gui.metric, gui.normalize]),\n",
    "        widgets.HBox([gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "try:\n",
    "    display_rake_gui(current_corpus(), language='english')\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color: green'>PREPARE/DESCRIBE </span>TextRank/SingleRank <span style='float: right; color: green'>OPTIONAL</span>\n",
    "\n",
    "https://github.com/JRC1995/TextRank-Keyword-Extraction\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "\n",
    "def display_document_key_terms_gui(corpus, document_index):\n",
    "    \n",
    "    lw = lambda width: widgets.Layout(width=width)\n",
    "    \n",
    "    methods = { 'RAKE': textacy_rake, 'SingleRank': textacy.keyterms.singlerank, 'TextRank': textacy.keyterms.textrank }\n",
    "    \n",
    "    # FIXME VARYING ASPECT: Add \"document_name\" to document_index, or function that creates name\n",
    "    filenames = document_index.filename\n",
    "    document_options =  [('All Documents', None)] + list(sorted(zip(filenames,filenames.index), key=lambda x: x[0]))\n",
    "\n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        progress=widgets.IntProgress(min=0, max=1, step=1, layout=lw(width='95%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=100, step=1, layout=lw('240px')),\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=lw(width='40%')),\n",
    "        method=widgets.Dropdown(description='Algorithm', options=[ 'RAKE', 'TextRank', 'SingleRank' ], value='TextRank', layout=lw('180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=lw('160px')),\n",
    "        left=widgets.Button(description='<<', button_style='Success', layout=lw('40px')),\n",
    "        right=widgets.Button(description='>>', button_style='Success', layout=lw('40px')),\n",
    "    )\n",
    "    \n",
    "    def get_keyterms(method, doc, normalize, n_keyterms):\n",
    "        keyterms = methods[method](doc, normalize=normalize, n_keyterms=n_keyterms)\n",
    "        terms = ', '.join([ x for x, y in keyterms ])\n",
    "        gui.progress.value += 1\n",
    "        return terms\n",
    "    \n",
    "    def get_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        document_ids = [ document_id ] if document_id is not None else [ doc._.meta['document_id'] for doc in corpus ]\n",
    "        gui.progress.value = 0\n",
    "        gui.progress.max = len(document_ids)\n",
    "        keyterms = [\n",
    "            get_keyterms(method, textacy_utility.get_document_by_id(corpus, document_id), normalize, n_keyterms) for document_id in document_ids\n",
    "        ]\n",
    "        df = pd.DataFrame({ 'document_id': document_ids, 'keyterms': keyterms}).set_index('document_id')\n",
    "        \n",
    "        # Add columns from document index\n",
    "        df_extended = domain_logic.add_domain_attributes(df, document_index)\n",
    "        \n",
    "        gui.progress.value = 0\n",
    "        return df_extended\n",
    "\n",
    "    def display_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df = get_document_key_terms(corpus, method, document_id, normalize, n_keyterms)\n",
    "            if len(df) == 1:\n",
    "                print(df.iloc[0]['keyterms'])\n",
    "            else:\n",
    "                display(df)\n",
    "            \n",
    "    itw = widgets.interactive(\n",
    "        display_document_key_terms,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        method=gui.method,\n",
    "        document_id=gui.document_id,\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([gui.document_id, gui.left, gui.right, gui.method, gui.normalize, gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "# FIXME: Use case specific logic\n",
    "corpus = current_corpus()\n",
    "document_index = domain_logic.compile_documents(corpus)\n",
    "\n",
    "display_document_key_terms_gui(corpus, document_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Clean Up the Text <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':'tight'}\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    try:\n",
    "        x, y = list(data[0]), list(data[1])\n",
    "        labels = x\n",
    "        #plt.figure(figsize=(8, 9 / 1.618))\n",
    "        plt.plot(x, y, 'ro', **kwargs)\n",
    "        plt.xticks(x, labels, rotation='75')\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        #plt.show()\n",
    "    except IndexError:\n",
    "        print('No data')\n",
    "        \n",
    "def display_cleaned_up_text(container, gui, display_type, document_id, term_substitutions, word_count_scores, word_document_count_scores, **opts): \n",
    "    \n",
    "    try:\n",
    "        def get_merged_words(scores, low, high):\n",
    "            ws = set([])\n",
    "            for x, wl in scores.items():\n",
    "                if low <= x <= high:\n",
    "                    ws.update(wl)\n",
    "            return ws\n",
    "\n",
    "        corpus = container.textacy_corpus\n",
    "                \n",
    "        doc = textacy_utility.get_document_by_id(corpus, document_id)\n",
    "\n",
    "        if doc is None:\n",
    "            return\n",
    "        \n",
    "        gui.output_text.clear_output()\n",
    "        gui.output_statistics.clear_output()\n",
    "        clear_output()\n",
    "        \n",
    "        if display_type.startswith('source_text'):\n",
    "\n",
    "            source_files = {\n",
    "                'source_text_raw': { 'filename': container.source_path, 'description': 'Raw text from PDF: Automatic text extraction using pdfminer Python package. ' },\n",
    "                'source_text_edited': { 'filename': container.source_path, 'description': 'Manually edited text: List of references, index, notes and page headers etc. removed.' },\n",
    "                'source_text_preprocessed': { 'filename': container.prepped_source_path, 'description': 'Preprocessed text: Normalized whitespaces. Unicode fixes. Urls, emails and phonenumbers removed. Accents removed.' }\n",
    "            }        \n",
    "\n",
    "            source_filename = source_files[display_type]['filename']\n",
    "            description =  source_files[display_type]['description']\n",
    "            text = utility.zip_get_text(source_filename, doc._.meta['filename'])\n",
    "\n",
    "            with gui.output_text:\n",
    "                print('[ ' + description.upper() + ' ]')\n",
    "                print(text)\n",
    "            return\n",
    "\n",
    "        with gui.output_text:\n",
    "\n",
    "            normalize = opts['normalize'] or 'orth'\n",
    "\n",
    "            extra_stop_words = set([])\n",
    "            if opts['min_freq'] > 1:\n",
    "                extra_stop_words.update(get_merged_words(word_count_scores[normalize], 1, opts['min_freq']))\n",
    "\n",
    "            if opts['max_doc_freq'] < 100:\n",
    "                extra_stop_words.update(get_merged_words(word_document_count_scores[normalize], opts['max_doc_freq'], 100))\n",
    "\n",
    "            extract_args = dict(\n",
    "                args=dict(\n",
    "                    ngrams=opts['ngrams'],\n",
    "                    named_entities=opts['named_entities'],\n",
    "                    normalize=opts['normalize'],\n",
    "                    as_strings=True\n",
    "                ),\n",
    "                kwargs=dict(\n",
    "                    min_freq=opts['min_freq'],\n",
    "                    include_pos=opts['include_pos'],\n",
    "                    filter_stops=opts['filter_stops'],\n",
    "                    filter_punct=opts['filter_punct']\n",
    "                ),\n",
    "                extra_stop_words=extra_stop_words,\n",
    "                substitutions=(term_substitutions if opts['substitute_terms'] else None),\n",
    "            )\n",
    "\n",
    "            terms = [ x for x in textacy_utility.extract_document_terms(doc, extract_args)]\n",
    "\n",
    "            if len(terms) == 0:\n",
    "                print(\"No text. Please change selection.\")\n",
    "\n",
    "        if display_type in ['sanitized_text', 'statistics' ]:\n",
    "                \n",
    "            if display_type == 'sanitized_text':\n",
    "                \n",
    "                with gui.output_text:\n",
    "                    print(' '.join([ t.replace(' ', '_') for t in terms ]))\n",
    "                    return\n",
    "\n",
    "            if display_type == 'statistics':\n",
    "\n",
    "                wf = nltk.FreqDist(terms)\n",
    "                gui.output_statistics.clear_output()\n",
    "                with gui.output_statistics:\n",
    "\n",
    "                    plt.figure(figsize=(18, 9 / 1.618))\n",
    "                    \n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    wf = nltk.FreqDist([len(x) for x in terms])\n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "\n",
    "                    #df = pd.DataFrame(wf.most_common(250), columns=['token','count'])\n",
    "                    #print('Token count: {} Vocab count: {}'.format(wf.N(), wf.B()))\n",
    "                    #display(df)\n",
    "\n",
    "\n",
    "    except Exception as ex:\n",
    "        with gui.output_text:\n",
    "            logger.error(ex)\n",
    "            \n",
    "def display_cleanup_text_gui(container, document_index):\n",
    "    \n",
    "    lw = lambda width: widgets.Layout(width=width)\n",
    "    \n",
    "    logger.info('Preparing corpus statistics...')\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    # FIXME VARYING ASPECT: Add \"document_name\" to document_index, or function that creates name\n",
    "    filenames = document_index.filename\n",
    "    document_options =  list(sorted(zip(filenames, filenames.index), key=lambda x: x[0]))\n",
    "    #document_options = gui_utility.get_treaty_dropdown_options(wti_index, corpus)\n",
    "    \n",
    "    logger.info('...loading term substitution mappings...')\n",
    "    subst_filename = os.path.join(domain_logic.DATA_FOLDER, 'term_substitutions.txt')\n",
    "    term_substitutions = textacy_utility.load_term_substitutions(subst_filename, default_term='_masked_', delim=';', vocab=corpus.spacy_lang.vocab)\n",
    "                                                                 \n",
    "    #pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = [('(All)', None)] + sorted([(k + ' (' + v + ')', k) for k,v in pos_tags.items() ])\n",
    "    display_options = {\n",
    "        'Source text (raw)': 'source_text_raw',\n",
    "        'Source text (edited)': 'source_text_edited',\n",
    "        'Source text (processed)': 'source_text_preprocessed',\n",
    "        'Sanitized text': 'sanitized_text',\n",
    "        'Statistics': 'statistics'\n",
    "    }\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        document_id=widgets.Dropdown(description='Document', options=document_options, value=document_options[1][1], layout=lw('400px')),\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1, layout=widgets.Layout(width='400px')),\n",
    "        max_doc_freq=widgets.IntSlider(description='Min doc. %', min=75, max=100, value=100, step=1, layout=widgets.Layout(width='400px')),\n",
    "        substitute_terms=widgets.ToggleButton(value=False, description='Mask GPE',  tooltip='Substitute terms as specified in file', icon='check'),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ None, 'lemma', 'lower' ], value='lower', layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        display_type=widgets.Dropdown(description='Show', value='statistics', options=display_options, layout=widgets.Layout(width='180px')),\n",
    "        left=widgets.Button(description='<', button_style='Success', layout=lw('40px')),\n",
    "        right=widgets.Button(description='>', button_style='Success', layout=lw('40px')),\n",
    "        output_text=widgets.Output(), # layout={'height': '500px'}),\n",
    "        output_statistics=widgets.Output(),\n",
    "        boxes=None\n",
    "    )\n",
    "    \n",
    "    logger.info('...word counts...')\n",
    "    word_count_scores = dict(\n",
    "        lemma=textacy_utility.generate_word_count_score(corpus, 'lemma', gui.min_freq.max),\n",
    "        lower=textacy_utility.generate_word_count_score(corpus, 'lower', gui.min_freq.max),\n",
    "        orth=textacy_utility.generate_word_count_score(corpus, 'orth', gui.min_freq.max)\n",
    "    )\n",
    "    logger.info('...word document count...')\n",
    "    word_document_count_scores = dict(\n",
    "        lemma=textacy_utility.generate_word_document_count_score(corpus, 'lemma', gui.max_doc_freq.min),\n",
    "        lower=textacy_utility.generate_word_document_count_score(corpus, 'lower', gui.max_doc_freq.min),\n",
    "        orth=textacy_utility.generate_word_document_count_score(corpus, 'orth', gui.max_doc_freq.min)\n",
    "    )\n",
    "\n",
    "    logger.info('...done!')\n",
    "    opts = dict(\n",
    "        min_freq=gui.min_freq,\n",
    "        max_doc_freq=gui.max_doc_freq,\n",
    "        substitute_terms=gui.substitute_terms,\n",
    "        ngrams=gui.ngrams,\n",
    "        min_word=gui.min_word,\n",
    "        normalize=gui.normalize,\n",
    "        filter_stops=gui.filter_stops,\n",
    "        filter_punct=gui.filter_punct,\n",
    "        named_entities=gui.named_entities,\n",
    "        include_pos=gui.include_pos\n",
    "    )\n",
    "    uix = widgets.interactive(\n",
    "        display_cleaned_up_text,\n",
    "        container=widgets.fixed(container),\n",
    "        gui=widgets.fixed(gui),\n",
    "        display_type=gui.display_type,\n",
    "        document_id=gui.document_id,\n",
    "        term_substitutions=widgets.fixed(term_substitutions),\n",
    "        word_count_scores=widgets.fixed(word_count_scores),\n",
    "        word_document_count_scores=widgets.fixed(word_document_count_scores),\n",
    "        **opts\n",
    "    )\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                widgets.HBox([gui.document_id,gui.left, gui.right]),\n",
    "                widgets.HBox([gui.display_type, gui.normalize]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq,\n",
    "                gui.max_doc_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            gui.output_statistics, gui.output_text\n",
    "        ]),\n",
    "        uix.children[-1]\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        #gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        #gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "    \n",
    "    uix.update()\n",
    "    return gui, uix\n",
    "\n",
    "try:\n",
    "    # FIXME VARYING ASPECTs: document_index/WTI_INDEX\n",
    "    document_index = domain_logic.compile_documents(current_corpus())\n",
    "    xgui, xuix = display_cleanup_text_gui(current_corpus_container(), document_index)\n",
    "except Exception as ex:\n",
    "    #raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> Most Discriminating Terms<span style='color: blue; float: right'>OPTIONAL</span>\n",
    "References\n",
    "King, Gary, Patrick Lam, and Margaret Roberts. “Computer-Assisted Keyword and Document Set Discovery from Unstructured Text.” (2014). http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.458.1445&rep=rep1&type=pdf.\n",
    "\n",
    "Displays the *most discriminating words* between two sets of treaties. Each treaty group can be filtered by country and period (signed year). In this way, the same group of countries can be studied for different time periods, or different groups of countries can be studied for the same time period. If \"Closed region\" is checked then **both** parties must be to the selected set of countries, from each region. In this way, one can for instance compare treaties signed between countries within the WTI group \"Communists\", against treaties signed within \"Western Europe\". \n",
    "\n",
    "<b>#terms</b> The number of most discriminating terms to return for each group.<br>\n",
    "<b>#top</b> Only terms with a frequency within the top #top terms out of all terms<br>\n",
    "<b>Closed region</b> If checked, then <u>both</u> treaty parties must be within selected region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: VARYING ASPECTS: Entire code base should be use case specific (generalization gives to much OH)\n",
    "import most_discriminating_terms_gui\n",
    "try:\n",
    "    # FIXME VARYING ASPECTs: document_index/WTI_INDEX\n",
    "    document_index = domain_logic.compile_documents(current_corpus())\n",
    "    most_discriminating_terms_gui.display_gui(document_index, current_corpus(), domain_logic=domain_logic)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> Corpus Statistics<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> List of Most Frequent Words<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import attrs\n",
    "\n",
    "def textacy_doc_to_bow(doc, target='lemma', weighting='count', as_strings=False, include=None):\n",
    "\n",
    "    spacy_store = doc.vocab.strings\n",
    "    \n",
    "    weighing_keys = { 'count', 'freq' }\n",
    "    target_keys = { 'lemma': attrs.LEMMA, 'lower': attrs.LOWER, 'orth': attrs.ORTH }\n",
    "    \n",
    "    default_exclude = lambda x: x.is_stop or x.is_punct or x.is_space\n",
    "    exclude = default_exclude if include is None else lambda x: x.is_stop or x.is_punct or x.is_space or not include(x)\n",
    "    \n",
    "    assert weighting in weighing_keys\n",
    "    assert target in target_keys\n",
    "\n",
    "    target_weights = doc.count_by(target_keys[target], exclude=exclude)\n",
    "    n_tokens = doc._.n_tokens\n",
    "\n",
    "    if weighting == 'count':\n",
    "        n_tokens = 1\n",
    "        \n",
    "    if as_strings:\n",
    "        bow = { spacy_store[word_id]: count / n_tokens for word_id, count in target_weights.items() }\n",
    "    else:\n",
    "        bow = target_weights\n",
    "        \n",
    "    return bow\n",
    "\n",
    "def qdisplay(x):\n",
    "    try:\n",
    "        display(x)\n",
    "    except:\n",
    "        print(ex)\n",
    "        \n",
    "def compute_list_of_most_frequent_words(\n",
    "    corpus,\n",
    "    documents,\n",
    "    gui,\n",
    "    group_by_columns=None,\n",
    "    #parties=None,\n",
    "    document_filters=None,\n",
    "    target='lemma',\n",
    "    weighting='count',\n",
    "    include_pos=None,\n",
    "    stop_words=None,\n",
    "    display_score=False\n",
    "):\n",
    "    stop_words = stop_words or set()\n",
    "    \n",
    "    def include(token):\n",
    "        flag = True\n",
    "        if not include_pos is None:\n",
    "             flag = flag and token.pos_ in include_pos\n",
    "        flag = flag and token.lemma_ not in stop_words\n",
    "        return flag\n",
    "    \n",
    "    gui.progress.max = len(corpus)\n",
    "    \n",
    "    df_freqs = None # pd.DataFrame({ 'document_id': [], 'token': [], 'score': [] })\n",
    "    \n",
    "    # parties_set = set(parties or [])\n",
    "    \n",
    "    # docs = corpus if len(parties_set) == 0 \\\n",
    "        #  else ( x for x in corpus if len(set((x._.meta['party1'], x._.meta['party2'])) & parties_set) > 0 )\n",
    "\n",
    "    docs = gui_utility.get_documents_by_field_filters(corpus, documents, document_filters)\n",
    "    \n",
    "    data = []\n",
    "    for doc in docs:\n",
    "        document_id = doc._.meta['document_id']\n",
    "        doc_freqs = textacy_doc_to_bow(doc, target=target, weighting=weighting, as_strings=False, include=include)\n",
    "        data.extend([(document_id, token, score) for token, score in doc_freqs.items() ])\n",
    "        gui.progress.value = gui.progress.value + 1\n",
    "        \n",
    "    df_freqs = pd.DataFrame(data, columns=['document_id', 'token_id', 'score'])\n",
    "    \n",
    "    df_freqs = pd.merge(df_freqs, documents, left_on='document_id', right_index=True, how='inner')\n",
    "       \n",
    "    df_freqs = df_freqs\\\n",
    "        .groupby(group_by_columns + ['token_id'])\\\n",
    "        .sum()\\\n",
    "        .reset_index()[group_by_columns + ['token_id', 'score']]\n",
    "    \n",
    "    df_freqs['position'] = df_freqs\\\n",
    "        .sort_values(by=group_by_columns + ['score'], ascending=False)\\\n",
    "        .groupby(group_by_columns)\\\n",
    "        .cumcount() + 1\n",
    "    \n",
    "    df_freqs = df_freqs[df_freqs.position < 500]\n",
    "    \n",
    "    string_store = corpus[0].vocab.strings\n",
    "    df_freqs['token'] =  df_freqs.token_id.apply(lambda x: string_store[x])\n",
    "    \n",
    "    df_freqs['term'] = df_freqs.token # if True else df_freqs.token\n",
    "    \n",
    "    if display_score is True:\n",
    "        df_freqs['term'] = df_freqs.term + '*' + (df_freqs.score.apply('{:,.3f}'.format) if weighting == 'freq' else df_freqs.score.astype(str))\n",
    "\n",
    "    gui.progress.value = 0\n",
    "    \n",
    "    return df_freqs\n",
    "    \n",
    "def display_list_of_most_frequent_words(gui, df):\n",
    "    if gui.output_type.value == 'table':\n",
    "        display(df)\n",
    "    elif gui.output_type.value == 'rank':\n",
    "        group_by_columns = gui.group_by_columns.value\n",
    "        df = df[df.position <= gui.n_tokens.value]\n",
    "        df_unstacked_freqs = df[group_by_columns + ['position', 'term']].set_index(group_by_columns + ['position']).unstack()\n",
    "        display(df_unstacked_freqs)\n",
    "    else:\n",
    "        filename = './word_trend_data.xlsx'\n",
    "        df.to_excel(filename)\n",
    "        print('Excel written: ' + filename)\n",
    "\n",
    "\n",
    "def word_frequency_gui(documents, corpus, compute_callback, display_callback, filter_options, group_by_options, enable_extra_stopwords):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    include_pos_tags = [ 'ADJ', 'VERB', 'NUM', 'ADV', 'NOUN', 'PROPN' ]\n",
    "    weighting_options = { 'Count': 'count', 'Frequency': 'freq' }\n",
    "    normalize_options = { '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }\n",
    "    #pos_tags = DF_TAGSET[DF_TAGSET.POS.isin(include_pos_tags)].groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    #pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    pos_options = include_pos_tags\n",
    "    \n",
    "    #counter = collections.Counter(corpus.word_counts(normalize='lemma', weighting='count', as_strings=True))\n",
    "    default_include_pos = ['NOUN', 'PROPN']\n",
    "    \n",
    "    frequent_words = []\n",
    "    if enable_extra_stopwords:\n",
    "        frequent_words = [ x[0] for x in textacy_utility.get_most_frequent_words(corpus, 100, include_pos=default_include_pos) ]\n",
    "\n",
    "    #group_by_options = { 'Year': 'year', 'Pope': 'pope', 'Genre': 'genre' } #TREATY_TIME_GROUPINGS[k]['title']: k for k in TREATY_TIME_GROUPINGS }\n",
    "    output_type_options = [ ( 'List', 'table' ), ( 'Rank', 'rank' ), ( 'Excel', 'excel' ), ]\n",
    "    ngrams_options = { '-': None, '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    \n",
    "    #party_preset_options = document_index.get_party_preset_options()\n",
    "    #parties_options = [ x for x in document_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    document_filters = gui_utility.generate_field_filters(documents, filter_options)\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=lw('98%')),\n",
    "        #parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('200px')),\n",
    "        #party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('200px')),\n",
    "        document_filters=document_filters,\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=None, layout=lw('200px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=lw('200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=lw('200px')),\n",
    "        weighting=widgets.Dropdown(description='Weighting', options=weighting_options, value='freq', layout=lw('200px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=default_include_pos, rows=7, layout=lw('150px')),\n",
    "        stop_words=widgets.SelectMultiple(description='STOP', options=frequent_words, value=list([]), rows=7, layout=lw('200px')),\n",
    "        group_by_columns=widgets.Dropdown(description='Group by', value=group_by_options[0][1], options=group_by_options, layout=lw('200px')),\n",
    "        output_type=widgets.Dropdown(description='Output', value='rank', options=output_type_options, layout=lw('200px')),\n",
    "        n_tokens=widgets.IntSlider(description='#tokens', value=25, min=3, max=500, layout=lw('250px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('120px')),\n",
    "        display_score=widgets.ToggleButton(description='Display score', icon='check', value=False, layout=lw('120px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.weighting,\n",
    "                gui.group_by_columns,\n",
    "                gui.output_type,\n",
    "            ]),\n",
    "            widgets.VBox([ x['widget'] for x in gui.document_filters]),\n",
    "            gui.include_pos,\n",
    "            gui.stop_words,\n",
    "            widgets.VBox([\n",
    "                gui.n_tokens,\n",
    "                gui.display_score,\n",
    "                gui.compute,\n",
    "            ], layout=widgets.Layout(align_items='flex-end')),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    #def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "    #    if gui.party_preset.value is None:\n",
    "    #        return\n",
    "    #    gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    #gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "    \n",
    "    if enable_extra_stopwords:\n",
    "        def pos_change_handler(*args):\n",
    "            with gui.output:\n",
    "                gui.compute.disabled = True\n",
    "                selected = set(gui.stop_words.value)\n",
    "                frequent_words = [\n",
    "                    x[0] for x in textacy_utility.get_most_frequent_words(\n",
    "                        corpus,\n",
    "                        100,\n",
    "                        normalize=gui.normalize.value,\n",
    "                        include_pos=gui.include_pos.value,\n",
    "                        weighting=gui.weighting.value\n",
    "                    )\n",
    "                ]\n",
    "                gui.stop_words.options = frequent_words\n",
    "                selected = selected & set(gui.stop_words.options)\n",
    "                gui.stop_words.value = list(selected)\n",
    "                gui.compute.disabled = False\n",
    "\n",
    "        gui.include_pos.observe(pos_change_handler, 'value')    \n",
    "        gui.weighting.observe(pos_change_handler, 'value')    \n",
    "    \n",
    "    def compute_callback_handler(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            try:\n",
    "                gui.compute.disabled = True\n",
    "                df_freqs = compute_callback(\n",
    "                    corpus=corpus,\n",
    "                    gui=gui,\n",
    "                    documents=documents,\n",
    "                    target=gui.normalize.value,\n",
    "                    group_by_columns=gui.group_by_columns.value,\n",
    "                    #parties=gui.parties.value,\n",
    "                    document_filters=[ (x['field'], x['widget'].value) for x in gui.document_filters],\n",
    "                    weighting=gui.weighting.value,\n",
    "                    include_pos=gui.include_pos.value,\n",
    "                    stop_words=set(gui.stop_words.value if enable_extra_stopwords else []),\n",
    "                    display_score=gui.display_score.value\n",
    "                )\n",
    "                display_callback(gui, df_freqs)\n",
    "            finally:\n",
    "                gui.compute.disabled = False\n",
    "\n",
    "    gui.compute.on_click(compute_callback_handler)\n",
    "    return gui\n",
    "                \n",
    "try:\n",
    "    document_index = domain_logic.compile_documents(current_corpus())\n",
    "    word_frequency_gui(\n",
    "        document_index,\n",
    "        current_corpus(),\n",
    "        compute_callback=compute_list_of_most_frequent_words,\n",
    "        display_callback=display_list_of_most_frequent_words,\n",
    "        filter_options=domain_logic.DOCUMENT_FILTERS,\n",
    "        group_by_options=domain_logic.GROUP_BY_OPTIONS,\n",
    "        enable_extra_stopwords=False\n",
    "    )\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> Corpus and Document Sizes<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import attrs\n",
    "%matplotlib inline\n",
    "\n",
    "def compute_corpus_statistics(\n",
    "    data_folder,\n",
    "    document_index,\n",
    "    container,\n",
    "    gui,\n",
    "    group_by_columns=None,\n",
    "    document_filters=None,\n",
    "    target='lemma',\n",
    "    include_pos=None,\n",
    "    stop_words=None\n",
    "):\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "\n",
    "    value_columns = list(textacy_utility.POS_NAMES) if (len(include_pos or [])) == 0 else list(include_pos)\n",
    "\n",
    "    corpus_documants = gui_utility.get_documents_by_field_filters(corpus, document_index, document_filters)\n",
    "    \n",
    "    documents = textacy_utility.get_corpus_data(corpus_documants, document_index, title='filename', columns_of_interest=None)\n",
    "\n",
    "    documents['lustrum'] = (documents.year - documents.year.mod(5)).astype(int) \n",
    "    documents['decade'] = (documents.year - documents.year.mod(10)).astype(int)\n",
    "    documents['total'] = documents[value_columns].apply(sum, axis=1)\n",
    "\n",
    "    aggregates = { x: ['sum'] for x in value_columns }\n",
    "    aggregates['total'] = ['sum', 'mean', 'min', 'max', 'size' ]\n",
    "\n",
    "    documents = documents.groupby(group_by_columns).agg(aggregates)\n",
    "    documents.columns = [ ('Total, ' + x[1].lower()) if x[0] == 'total' else x[0] for x in documents.columns ]\n",
    "    columns = sorted(value_columns) + sorted([ x for x in documents.columns if x.startswith('Total')])\n",
    "    return documents[columns]\n",
    "        \n",
    "def corpus_statistics_gui(data_folder, document_index, container, compute_callback, display_callback, filter_options, group_by_options):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    include_pos_tags =  list(textacy_utility.POS_NAMES)\n",
    "    pos_options = include_pos_tags\n",
    "    \n",
    "    counter = collections.Counter(corpus.word_counts(normalize='lemma', weighting='count', as_strings=True))\n",
    "    frequent_words = [ x[0] for x in textacy_utility.get_most_frequent_words(corpus, 100) ]\n",
    "\n",
    "    #group_by_options = { TREATY_TIME_GROUPINGS[k]['title']: k for k in TREATY_TIME_GROUPINGS }\n",
    "    # output_type_options = [ ( 'Table', 'table' ), ( 'Pivot', 'pivot' ), ( 'Excel', 'excel' ), ]\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    #party_preset_options = wti_index.get_party_preset_options()\n",
    "    #parties_options = [ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    document_filters = gui_utility.generate_field_filters(document_index, filter_options)\n",
    "    gui = types.SimpleNamespace(\n",
    "        #parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('180px')),\n",
    "        #party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('200px')),\n",
    "        document_filters=document_filters,\n",
    "        target=widgets.Dropdown(description='Normalize', options={ '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }, value='lemma', layout=lw('200px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list([]), rows=7, layout=widgets.Layout(width='180px')),\n",
    "        group_by_columns=widgets.Dropdown(description='Group by', value=group_by_options[0][1], options=group_by_options, layout=lw('200px')),\n",
    "        #output_type=widgets.Dropdown(description='Output', value='table', options=output_type_options, layout=widgets.Layout(width='200px')),\n",
    "        compute=widgets.Button(description='Compute', layout=lw('120px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    \n",
    "    boxes = widgets.VBox([\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.group_by_columns,\n",
    "                #gui.party_preset,\n",
    "            ]),\n",
    "            #widgets.VBox([gui.parties,]),\n",
    "            widgets.VBox([ x['widget'] for x in gui.document_filters]),\n",
    "            gui.include_pos,\n",
    "            widgets.VBox([\n",
    "                gui.compute\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    #def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "    #    if gui.party_preset.value is None:\n",
    "    #        return\n",
    "    #    gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    #gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "\n",
    "    def compute_callback_handler(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df_freqs = compute_callback(\n",
    "                data_folder=data_folder,\n",
    "                document_index=document_index,\n",
    "                container=container,\n",
    "                gui=gui,\n",
    "                target=gui.target.value,\n",
    "                group_by_columns=gui.group_by_columns.value,\n",
    "                #parties=gui.parties.value,\n",
    "                document_filters=[ (x['field'], x['widget'].value) for x in gui.document_filters],\n",
    "                include_pos=gui.include_pos.value,\n",
    "            )\n",
    "            display_callback(gui, df_freqs)\n",
    "\n",
    "    gui.compute.on_click(compute_callback_handler)\n",
    "    return gui\n",
    "\n",
    "def plot_simple(xs, ys, **figopts):\n",
    "    source = bokeh.models.ColumnDataSource(dict(x=xs, y=ys))\n",
    "    figopts = utility.extend(dict(title='', toolbar_location=\"right\"), figopts)\n",
    "    p = bokeh.plotting.figure(**figopts)\n",
    "    glyph = p.line(source=source, x='x', y='y', line_color=\"#b3de69\")\n",
    "    return p\n",
    "\n",
    "def display_corpus_statistics(gui, df):\n",
    "    display(df)\n",
    "    #with gui.output:\n",
    "    #    plotopts=dict(plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "    #    p = plot_simple(X.index, X['Total, mean'], **plotopts)\n",
    "    #    bokeh.plotting.show(p)\n",
    "\n",
    "try:\n",
    "    document_index = domain_logic.compile_documents(current_corpus())\n",
    "    gui = corpus_statistics_gui(\n",
    "        domain_logic.DATA_FOLDER,\n",
    "        document_index,\n",
    "        current_corpus_container(),\n",
    "        compute_callback=compute_corpus_statistics,\n",
    "        display_callback=display_corpus_statistics,\n",
    "        filter_options=domain_logic.DOCUMENT_FILTERS,\n",
    "        group_by_options=domain_logic.GROUP_BY_OPTIONS\n",
    "    )\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

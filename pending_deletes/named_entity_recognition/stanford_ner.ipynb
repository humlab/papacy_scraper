{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, io, codecs, time\n",
    "import collections\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import StanfordTokenizer\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "def extract_entity_phrases(data, classes = [ 'LOCATION', 'PERSON']):\n",
    "\n",
    "    # Extract entities of selected classes, add index to enable merge to phrases\n",
    "    entities = [ (i, word, wclass)\n",
    "        for (i, (word, wclass)) in enumerate(data)\n",
    "            if wclass in classes ]\n",
    "\n",
    "    # Merge adjacent entities having the same classifier\n",
    "    for i in range(len(entities) - 1, 0, -1):\n",
    "        if entities[i][0] == entities[i-1][0] + 1 and entities[i][2] == entities[i-1][2]:\n",
    "            entities[i-1] = (entities[i-1][0], entities[i-1][1] + \" \" + entities[i][1], entities[i-1][2])\n",
    "            del entities[i]\n",
    "\n",
    "    # Remove index in returned data\n",
    "    return [ (word, wclass) for (i, word, wclass) in entities  ]\n",
    "\n",
    "def extract_document_info(filename):\n",
    "    document_name = os.path.basename(os.path.splitext(filename)[0])\n",
    "    pope, lang, genre, *tail = document_name.split('_')\n",
    "    try:\n",
    "        year = next((x for x in tail if x.isnumeric() and len(x) == 4), '0')\n",
    "        if (year == '0'):\n",
    "            for item in tail:\n",
    "                item_split = re.split(r'[_\\-\\.]', item)\n",
    "                year = next((x[-4:] for x in item_split if x.isnumeric() and (len(x) == 4 or len(x) == 8)), '0')\n",
    "                if year != '0':\n",
    "                    break\n",
    "    except:\n",
    "        year = '0'\n",
    "        print('Parse YEAR failed: {0}'.format(filename))\n",
    "\n",
    "    return (document_name, pope, lang, genre, year)\n",
    "\n",
    "def create_ner_tagger(options):\n",
    "    return StanfordNERTagger(options['ner_model'], os.path.join(options[\"ner_path\"], \"stanford-ner.jar\"))\n",
    "\n",
    "def create_tokenizer(options):\n",
    "    return StanfordTokenizer()\n",
    "\n",
    "def read_file(filename):\n",
    "    with codecs.open(filename, \"r\", \"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def create_statistics(entities):\n",
    "    wc = collections.Counter()\n",
    "    wc.update(entities)\n",
    "    return wc\n",
    "\n",
    "def serialize_content(stats, filename, token_count):\n",
    "    document_name, pope, lang, genre, year = extract_document_info(filename)\n",
    "    data = [ (document_name, year, genre, pope, word, wclass, stats[(word, wclass)], token_count) for (word, wclass) in stats  ]\n",
    "    content = '\\n'.join(map(lambda x : ';'.join([str(y) for y in x]), data))\n",
    "    return content\n",
    "\n",
    "def write_content(outfile, content):\n",
    "    if content != '':\n",
    "        outfile.write(content)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "def main(options):\n",
    "\n",
    "    nerrer      = create_ner_tagger(options)\n",
    "    tokenizer   = create_tokenizer(options)\n",
    "    outfile     = os.path.join(options['output_folder'], \"output_\" + time.strftime(\"%Y%m%d_%H%M%S\") + \".csv\")\n",
    "\n",
    "    for zip_source in options[\"zip_sources\"]:\n",
    "        with io.open(outfile, 'w', encoding='utf8') as o:\n",
    "            with zipfile.ZipFile(zip_source) as pope_zip:\n",
    "                for filename in pope_zip.namelist():\n",
    "                    with pope_zip.open(filename) as pope_file:\n",
    "                        text = pope_file.read()\n",
    "                        tokens      = tokenizer.tokenize(text)\n",
    "                        data        = nerrer.tag(tokens)\n",
    "                        entities    = extract_entity_phrases(data,  [ 'LOCATION', 'PERSON', 'ORGANIZATION' ])\n",
    "                        statistics  = create_statistics(entities)\n",
    "                        content     = serialize_content(statistics, filename, len(tokens))\n",
    "                        write_content(o, content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    options = {\n",
    "        \"zip_sources\": [ 'treaty_text_corpora_20181206_preprocessed.zip' ],\n",
    "        \"ner_path\":  '/home/roger/source/stanford-ner-2018-10-16',\n",
    "        'ner_model': 'english.all.3class.distsim.crf.ser.gz',\n",
    "        'output_folder': '../data/'\n",
    "    }\n",
    "\n",
    "    os.environ['STANFORD_MODELS'] = os.path.join(options[\"ner_path\"], \"classifiers\")\n",
    "    os.environ['JAVAHOME'] = '/usr/lib/jvm/java-8-oracle'\n",
    "    os.environ['CLASSPATH'] = os.path.join(options[\"ner_path\"], \"stanford-ner.jar\") + \\\n",
    "        \";\"  + os.path.join(options[\"ner_path\"], \"stanford-postagger.jar\")\n",
    "\n",
    "    main(options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
